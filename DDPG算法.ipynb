{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 DDPG 算法训练 Pendulum-v1 环境\n",
    "\n",
    "**DDPG** 是 DPG 算法的一个重要改进，它通过引入 **经验回放 (Experience Replay)** 和 **目标网络 (Target Networks)** 解决了训练不稳定的问题，非常适合解决连续动作空间下的强化学习任务。它同样属于 Actor-Critic 框架：\n",
    "\n",
    "1.  **Actor (策略网络)**: 学习在给定状态下应该采取什么动作。\n",
    "2.  **Critic (价值网络)**: 评估在特定状态下采取某个动作的好坏（Q值）。\n",
    "3.  **Target Networks**: Actor 和 Critic 网络各自拥有一套目标网络，用于在计算目标Q值时提供稳定的目标，并通过“软更新”缓慢地同步参数。\n",
    "\n",
    "在本教程中，我们将使用 PyTorch 和 Gymnasium 来训练一个智能体，使其能够成功地将一个钟摆（Pendulum）竖立起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库\n",
    "\n",
    "首先，我们导入所有必需的库，包括 `gym` 用于创建环境，`torch` 用于构建和训练神经网络，以及 `numpy` 和 `random` 用于数据处理。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T07:55:45.384754Z",
     "start_time": "2025-07-30T07:55:40.426989Z"
    }
   },
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_ # 新版numpy把 np.bool8 指向 np.bool_\n",
    "import random\n",
    "from collections import deque\n",
    "import copy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 定义超参数和环境\n",
    "\n",
    "在这一步，我们定义所有训练过程中需要用到的超参数。相比于 DPG，我们新增了 `tau` 用于目标网络的软更新。\n",
    "\n",
    "- **`tau`**: 软更新系数。目标网络的参数将以 `(1-tau)` 的比例保留旧参数，并以 `tau` 的比例从主网络中更新，这使得目标网络的变化更加平滑。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T07:55:55.808401Z",
     "start_time": "2025-07-30T07:55:55.738299Z"
    }
   },
   "source": [
    "# 定义一些超参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env_name = \"Pendulum-v1\"\n",
    "gamma = 0.99\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "max_episodes = 500\n",
    "max_steps = 200\n",
    "exploration_noise = 0.1\n",
    "replay_buffer_size = 10000\n",
    "tau = 0.005 # 目标网络软更新系数\n",
    "\n",
    "# 创建环境\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_bound = env.action_space.high[0]\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"State Dimension: {state_dim}\")\n",
    "print(f\"Action Dimension: {action_dim}\")\n",
    "print(f\"Action Bound: {action_bound}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "State Dimension: 3\n",
      "Action Dimension: 1\n",
      "Action Bound: 2.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义 Actor-Critic 网络结构\n",
    "\n",
    "网络结构与 DPG 保持一致。我们将为 Actor 和 Critic 分别创建两套网络：一套是主网络（用于训练和决策），另一套是目标网络（用于稳定学习过程）。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T07:56:00.383402Z",
     "start_time": "2025-07-30T07:56:00.366596Z"
    }
   },
   "source": [
    "# 定义一个策略网络 (Actor)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * action_bound\n",
    "        return x\n",
    "\n",
    "# 定义一个值函数网络 (Critic)\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 初始化网络、优化器和回放缓冲区\n",
    "\n",
    "我们创建主网络和目标网络。目标网络的初始权重与主网络完全相同。然后为主网络定义优化器。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T07:56:09.902578Z",
     "start_time": "2025-07-30T07:56:06.888526Z"
    }
   },
   "source": [
    "# 主网络\n",
    "policy_net = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "value_net = ValueNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "# 目标网络\n",
    "policy_net_target = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "value_net_target = ValueNetwork(state_dim, action_dim).to(device)\n",
    "\n",
    "# 初始化目标网络权重\n",
    "policy_net_target.load_state_dict(policy_net.state_dict())\n",
    "value_net_target.load_state_dict(value_net.state_dict())\n",
    "\n",
    "# 优化器\n",
    "policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=lr)\n",
    "\n",
    "# 经验回放池\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 开始训练\n",
    "\n",
    "DDPG 的训练流程与 DPG 类似，但有关键区别：\n",
    "\n",
    "```\n",
    "对于每个 episode:\n",
    "  ...\n",
    "  对于每个 time step t:\n",
    "    ...\n",
    "    (学习) 如果缓冲区数据足够：\n",
    "       a. 从缓冲区随机采样一个 batch 的数据\n",
    "       b. (关键) 计算目标Q值: y_i = r_i + gamma * Q_critic_target(s_{i+1}, a_{i+1}) (其中 a_{i+1} = mu_actor_target(s_{i+1}))\n",
    "       c. 更新 Critic 主网络: 最小化 (y_i - Q_critic(s_i, a_i))^2\n",
    "       d. 更新 Actor 主网络: 使用采样策略梯度最大化 Q_critic(s_i, mu_actor(s_i))\n",
    "       e. (关键) 软更新目标网络: target_params = tau * main_params + (1-tau) * target_params\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T08:15:57.408639Z",
     "start_time": "2025-07-30T07:58:53.676551Z"
    }
   },
   "source": [
    "for i_episode in range(max_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # --- 采样 ---\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=device)\n",
    "        action_tensor = policy_net(state_tensor)\n",
    "        action = action_tensor.cpu().detach().numpy()\n",
    "        noise = np.random.normal(0, exploration_noise, size=action_dim)\n",
    "        action = np.clip(action + noise, -action_bound, action_bound)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # --- 学习 ---\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue\n",
    "\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.as_tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "        actions = torch.as_tensor(np.array(actions), dtype=torch.float32, device=device)\n",
    "        rewards = torch.as_tensor(np.array(rewards), dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "        dones = torch.as_tensor(np.array(dones), dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "\n",
    "        # --- 更新 Critic 网络 ---\n",
    "        with torch.no_grad():\n",
    "            # 使用目标网络计算下一个动作和下一个Q值\n",
    "            next_actions = policy_net_target(next_states)\n",
    "            next_q_values = value_net_target(next_states, next_actions)\n",
    "            target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "\n",
    "        current_q_values = value_net(states, actions)\n",
    "        value_loss = F.mse_loss(current_q_values, target_q_values)\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # --- 更新 Actor 网络 ---\n",
    "        policy_loss = -value_net(states, policy_net(states)).mean()\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # --- 软更新目标网络 ---\n",
    "        for param, target_param in zip(value_net.parameters(), value_net_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(policy_net.parameters(), policy_net_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if (i_episode + 1) % 10 == 0:\n",
    "        print(f\"Episode: {i_episode+1}, Reward: {episode_reward:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Reward: -1389.95\n",
      "Episode: 20, Reward: -755.39\n",
      "Episode: 30, Reward: -339.02\n",
      "Episode: 40, Reward: -128.58\n",
      "Episode: 50, Reward: -125.39\n",
      "Episode: 60, Reward: -247.05\n",
      "Episode: 70, Reward: -238.77\n",
      "Episode: 80, Reward: -238.34\n",
      "Episode: 90, Reward: -120.52\n",
      "Episode: 100, Reward: -226.88\n",
      "Episode: 110, Reward: -118.85\n",
      "Episode: 120, Reward: -119.55\n",
      "Episode: 130, Reward: -245.45\n",
      "Episode: 140, Reward: -118.59\n",
      "Episode: 150, Reward: -117.33\n",
      "Episode: 160, Reward: -249.02\n",
      "Episode: 170, Reward: -2.08\n",
      "Episode: 180, Reward: -123.55\n",
      "Episode: 190, Reward: -116.83\n",
      "Episode: 200, Reward: -117.11\n",
      "Episode: 210, Reward: -117.83\n",
      "Episode: 220, Reward: -118.58\n",
      "Episode: 230, Reward: -241.28\n",
      "Episode: 240, Reward: -1.22\n",
      "Episode: 250, Reward: -378.72\n",
      "Episode: 260, Reward: -370.25\n",
      "Episode: 270, Reward: -240.98\n",
      "Episode: 280, Reward: -116.54\n",
      "Episode: 290, Reward: -126.39\n",
      "Episode: 300, Reward: -121.86\n",
      "Episode: 310, Reward: -2.45\n",
      "Episode: 320, Reward: -245.27\n",
      "Episode: 330, Reward: -131.25\n",
      "Episode: 340, Reward: -256.59\n",
      "Episode: 350, Reward: -2.94\n",
      "Episode: 360, Reward: -249.44\n",
      "Episode: 370, Reward: -117.27\n",
      "Episode: 380, Reward: -5.75\n",
      "Episode: 390, Reward: -345.47\n",
      "Episode: 400, Reward: -119.55\n",
      "Episode: 410, Reward: -125.62\n",
      "Episode: 420, Reward: -235.41\n",
      "Episode: 430, Reward: -133.09\n",
      "Episode: 440, Reward: -130.94\n",
      "Episode: 450, Reward: -125.86\n",
      "Episode: 460, Reward: -121.70\n",
      "Episode: 470, Reward: -4.66\n",
      "Episode: 480, Reward: -4.98\n",
      "Episode: 490, Reward: -244.02\n",
      "Episode: 500, Reward: -134.92\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
