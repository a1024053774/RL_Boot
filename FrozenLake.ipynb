{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Gymnasium 进行 Q-Learning 训练 FrozenLake\n",
    "\n",
    "这是一个使用 Q-Learning 算法来训练一个智能体玩 FrozenLake-v1 游戏的示例，并包含对训练后智能体的评估。"
   ],
   "id": "9e41c2644e974ed6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入库"
   ],
   "id": "72cd7023cdd8957f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:53:55.169758Z",
     "start_time": "2025-07-28T11:53:55.166681Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm import tqdm"
   ],
   "id": "5b026fb553d15465",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 环境初始化\n",
    "\n",
    "我们加载 FrozenLake 环境，并查看其状态空间和动作空间。"
   ],
   "id": "f8b427c6efd6fe7f"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:53:58.606113Z",
     "start_time": "2025-07-28T11:53:58.599620Z"
    }
   },
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# 观察操作空间和状态空间\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n",
    "\n",
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n",
    "\n",
    "state_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "print(\"There are\", state_space, \"可能的状态\")\n",
    "print(\"There are\", action_space, \"可能的动作\")"
   ],
   "id": "70f9a203575dc53d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 4\n",
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n",
      "There are 16 possible states\n",
      "There are 4 possible actions\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数设定\n",
    "\n",
    "定义 Q-Learning 算法所需的超参数。"
   ],
   "id": "81aa2bfe33d88b76"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:54:18.390892Z",
     "start_time": "2025-07-28T11:54:18.387155Z"
    }
   },
   "source": [
    "n_training_episodes = 100000\n",
    "max_steps = 100        # 每个episode最多步数\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99\n",
    "\n",
    "# Epsilon-Greedy 策略的参数\n",
    "min_epsilon = 0.05\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.0005"
   ],
   "id": "3e77deab151e98aa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 辅助函数\n",
    "\n",
    "这里定义了初始化 Q 表、贪心策略和 Epsilon-Greedy 策略的函数。"
   ],
   "id": "1d858d653adf3853"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:54:31.803444Z",
     "start_time": "2025-07-28T11:54:31.799208Z"
    }
   },
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    \"\"\"初始化Q表为全零\"\"\"\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "def greedy_policy(Qtable, state):\n",
    "    \"\"\"在利用时，选择具有最高Q值的动作\"\"\"\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    \"\"\"在探索和利用之间进行权衡\"\"\"\n",
    "    random_num = random.uniform(0, 1)\n",
    "    if random_num > epsilon:\n",
    "        # 利用 (Exploitation)\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    else:\n",
    "        # 探索 (Exploration)\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ],
   "id": "46da1ab330e40cc8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q-Learning 训练函数"
   ],
   "id": "6644b0458a0f7b9c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:54:34.453233Z",
     "start_time": "2025-07-28T11:54:34.446927Z"
    }
   },
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    \"\"\"训练Q表\"\"\"\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # 在每个 episode 开始时，衰减 epsilon\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        \n",
    "        # 重置环境\n",
    "        state, info = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 1. 选择动作\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # 2. 在环境中执行动作\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # 3. 使用贝尔曼方程更新Q表\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (\n",
    "                reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]\n",
    "            )\n",
    "\n",
    "            # 如果 episode 结束，则中断循环\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "            # 更新状态\n",
    "            state = new_state\n",
    "            \n",
    "    return Qtable"
   ],
   "id": "c2e5abf5cebe4005",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 开始训练\n",
    "\n",
    "现在我们使用上面定义的函数来初始化Q表并开始训练过程。"
   ],
   "id": "ab6eba996f0ed994"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:54:50.485497Z",
     "start_time": "2025-07-28T11:54:38.133942Z"
    }
   },
   "source": [
    "# 初始化Q表\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "\n",
    "# 训练Q表\n",
    "Qtable_frozenlake = train(\n",
    "    n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake\n",
    ")"
   ],
   "id": "eefdfe673d874f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:12<00:00, 8099.76it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 查看训练后的Q表"
   ],
   "id": "39b04861bcfa8fe7"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:54:52.682566Z",
     "start_time": "2025-07-28T11:54:52.677062Z"
    }
   },
   "source": [
    "print(\"训练后的Q表：\")\n",
    "print(Qtable_frozenlake)"
   ],
   "id": "4497ab66fd3a394e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后的Q表：\n",
      "[[0.94148015 0.95099005 0.93206535 0.94148015]\n",
      " [0.94148015 0.         0.9226185  0.93205744]\n",
      " [0.93205181 0.7716989  0.42714575 0.73494114]\n",
      " [0.71697927 0.         0.19915692 0.24927304]\n",
      " [0.95099005 0.96059601 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.97932987 0.         0.64056064]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.970299   0.95099005]\n",
      " [0.96059601 0.9801     0.9801     0.        ]\n",
      " [0.97027784 0.99       0.         0.96603371]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.99       0.970299  ]\n",
      " [0.9801     0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 评估智能体 (Evaluate the Agent)\n",
    "\n",
    "训练完成后，我们定义一个函数来评估智能体的表现。这个过程不再进行探索（没有 epsilon-greedy），而是完全采用贪心策略（greedy policy），即在每个状态下都选择Q值最高的动作。"
   ],
   "id": "a2b7dd546dc48532"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:55:07.646404Z",
     "start_time": "2025-07-28T11:55:07.640161Z"
    }
   },
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \"\"\"评估智能体在 n_eval_episodes 次游戏中的平均表现\"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 完全根据Q表来选择最优动作 (Greedy policy)\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ],
   "id": "476989ac9b726c4e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 运行评估\n",
    "\n",
    "设置评估参数并运行评估函数。平均奖励（Mean_reward）为1.0代表智能体在每次评估中都能成功到达终点。"
   ],
   "id": "e24ed0ed23e2a700"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T11:55:11.695873Z",
     "start_time": "2025-07-28T11:55:11.557538Z"
    }
   },
   "source": [
    "# 设置评估参数\n",
    "n_eval_episodes = 1000  # 进行1000次评估\n",
    "# 为了评估的可复现性，我们使用固定的种子\n",
    "# 您也可以设置为 eval_seed = None 来进行完全随机的评估\n",
    "eval_seed = [i for i in range(n_eval_episodes)]\n",
    "\n",
    "# 评估智能体并打印结果\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"\\nMean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "id": "fc5750c821568bac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 7609.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
