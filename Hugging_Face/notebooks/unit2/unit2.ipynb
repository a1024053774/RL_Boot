{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# 第 2 单元：将 Q-Learning 算法应用于 FrozenLake-v1 ⛄ and Taxi-v3 🚕\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
    "\n",
    "在本 notebook 中，**你将从头开始编写你的第一个强化学习智能体**，使用 Q-Learning 算法来玩 FrozenLake ❄️，并将其与社区分享，还将尝试不同的配置。\n",
    "\n",
    "⬇️ 以下是你**在几分钟内就能实现**的示例。⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU_vXBrl1Jx"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###🎮 环境:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "###📚 RL 库:\n",
    "\n",
    "- Python and NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "我们一直在努力改进我们的教程，因此，**如果你在本 notebook 中发现任何问题**，请[在 GitHub 仓库中提出 issue](https://github.com/huggingface/deep-rl-class/issues)。"
   ],
   "metadata": {
    "id": "DPTBOv9HYLZ2"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## 本 notebook 的目标 🏆\n",
    "\n",
    "完成本 notebook 后，你将能够：\n",
    "\n",
    "- 能够使用 **Gymnasium**，即环境库。\n",
    "- 能够从头开始编写 Q-Learning 智能体。\n",
    "- 能够**将你训练好的智能体和代码推送到 Hub**，并附上精美的视频回放和评估分数 🔥。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 本 notebook 来自深度强化学习课程\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ],
   "metadata": {
    "id": "viNzVbVaYvY3"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "在这门免费课程中，你将：\n",
    "\n",
    "- 📖 从**理论和实践**两方面学习深度强化学习。\n",
    "- 🧑‍💻 学习**使用著名的深度强化学习库**，如 Stable Baselines3、RL Baselines3 Zoo、CleanRL 和 Sample Factory 2.0。\n",
    "- 🤖 在**独特的环境中训练智能体**\n",
    "\n",
    "更多内容请查看 📚 课程大纲 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "别忘了**<a href=\"http://eepurl.com/ic5ZUD\">报名参加课程</a>**（我们会收集你的电子邮件，以便在每个单元发布时**向你发送链接，并为你提供有关挑战和更新的信息）。**\n",
    "\n",
    "\n",
    "保持联系的最佳方式是加入我们的 Discord 服务器，与社区和我们交流 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## 先决条件 🏗️\n",
    "\n",
    "在深入学习本 notebook 之前，你需要：\n",
    "\n",
    "🔲 📚 **通过阅读第 2 单元来学习 [Q-Learning](https://huggingface.co/deep-rl-course/unit2/introduction)** 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ONOODsyrMU"
   },
   "source": [
    "## Q-Learning 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68VveLacfxJ"
   },
   "source": [
    "*Q-Learning* **是这样一种强化学习算法**：\n",
    "\n",
    "- 训练 *Q-Function*，这是一个**动作价值函数**，通过一个*Q-table* **在内存中进行编码，该 Q-table 包含所有状态-动作对的值**。\n",
    "\n",
    "- 给定一个状态和动作，我们的 Q-Function **将在 Q-table 中搜索相应的值。**\n",
    "    \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
    "\n",
    "- 训练完成后，**我们得到一个最优的 Q-Function，即一个最优的 Q-Table。**\n",
    "    \n",
    "- 如果我们**有一个最优的 Q-function**，我们就\n",
    "有了一个最优策略，因为我们**知道在每个状态下要采取的最佳动作。**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n",
    "\n",
    "\n",
    "但是，在开始时，我们的 **Q-Table 是无用的，因为它为每个状态-动作对提供任意值（大多数情况下，我们将 Q-Table 初始化为 0 值）**。但是，随着我们探索环境并更新我们的 Q-Table，它会为我们提供越来越好的近似值\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "这是 Q-Learning 的伪代码：\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 让我们编写第一个强化学习算法 🚀"
   ],
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "要通过[认证流程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)验证此实践，你需要将训练好的 Taxi 模型推送到 Hub，并**获得 >= 4.5 的结果**。\n",
    "\n",
    "要找到你的结果，请转到[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)并找到你的模型，**结果 = mean_reward - std of reward**\n",
    "\n",
    "有关认证流程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ],
   "metadata": {
    "id": "Kdxb1IhzTn0v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 安装依赖项并创建虚拟显示器 🔽\n",
    "\n",
    "在 notebook 中，我们需要生成一个回放视频。为此，在 Colab 中，**我们需要一个虚拟屏幕来渲染环境**（从而记录帧）。\n",
    "\n",
    "因此，以下单元格将安装库并创建和运行虚拟屏幕 🖥\n",
    "\n",
    "我们将安装多个库：\n",
    "\n",
    "- `gymnasium`：包含 FrozenLake-v1 ⛄ 和 Taxi-v3 🚕 环境。\n",
    "- `pygame`：用于 FrozenLake-v1 和 Taxi-v3 UI。\n",
    "- `numpy`：用于处理我们的 Q-table。\n",
    "\n",
    "Hugging Face Hub 🤗 是一个任何人都可以共享和探索模型和数据集的中心。它具有版本控制、指标、可视化和其他功能，可以让你轻松地与他人协作。\n",
    "\n",
    "你可以在此处查看所有可用的深度强化学习模型（如果它们使用 Q Learning） 👉 https://huggingface.co/models?other=q-learning"
   ],
   "metadata": {
    "id": "4gpxC1_kqUYe"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9XaULfDZDvrC",
    "ExecuteTime": {
     "end_time": "2025-08-09T12:52:43.400318Z",
     "start_time": "2025-08-09T12:51:56.946483Z"
    }
   },
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ],
   "metadata": {
    "id": "n71uTX7qqzz2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "为确保使用新安装的库，**有时需要重新启动 notebook 运行时**。下一个单元格将强制**运行时崩溃，因此你需要重新连接并从此处开始运行代码**。多亏了这个技巧，**我们将能够运行我们的虚拟屏幕。**"
   ],
   "metadata": {
    "id": "K6XC13pTfFiD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ],
   "metadata": {
    "id": "3kuZbWAkfHdg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 虚拟显示\n",
    "# from pyvirtualdisplay import Display\n",
    "#\n",
    "# virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ],
   "metadata": {
    "id": "DaY1N4dBrabi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-7f-Swax_9x"
   },
   "source": [
    "## 导入包 📦\n",
    "\n",
    "除了已安装的库之外，我们还使用：\n",
    "\n",
    "- `random`：生成随机数（对 epsilon-greedy 策略很有用）。\n",
    "- `imageio`：生成回放视频。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VcNvOAQlysBJ",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:28:58.129278Z",
     "start_time": "2025-08-09T13:28:57.915582Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import pickle # 新版python用直接用pickle\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "我们现在准备好编写我们的 Q-Learning 算法了 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "# 第 1 部分：冰湖 ⛄（非湿滑版本）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAvihuHdy9tw"
   },
   "source": [
    "## 创建并了解 [FrozenLake 环境 ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "\n",
    "💡 开始使用环境时，一个好习惯是查看其文档\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "\n",
    "我们将训练我们的 Q-Learning 智能体**从起始状态 (S) 导航到目标状态 (G)，只在冰冻的瓷砖 (F) 上行走，并避开洞 (H)**。\n",
    "\n",
    "我们可以有两种大小的环境：\n",
    "\n",
    "- `map_name=\"4x4\"`：一个 4x4 网格版本\n",
    "- `map_name=\"8x8\"`：一个 8x8 网格版本\n",
    "\n",
    "\n",
    "环境有两种模式：\n",
    "\n",
    "- `is_slippery=False`：由于冰湖的非湿滑特性，智能体始终**朝预定方向移动**（确定性）。\n",
    "- `is_slippery=True`：由于冰湖的湿滑特性，智能体**可能不会总是朝预定方向移动**（随机性）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "现在，让我们使用 4x4 地图和非湿滑版本来简化问题。\n",
    "我们添加一个名为 `render_mode` 的参数，它指定应如何可视化环境。在我们的情况下，因为我们**希望在最后录制环境的视频，所以需要将 render_mode 设置为 rgb_array**。\n",
    "\n",
    "正如[文档中所解释的](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)，“rgb_array”：返回表示环境当前状态的单个帧。帧是一个形状为 (x, y, 3) 的 np.ndarray，表示一个 x-by-y 像素图像的 RGB 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzJnb8O3y8up"
   },
   "outputs": [],
   "source": [
    "# 使用 4x4 地图和非湿滑版本以及 render_mode=\"rgb_array\" 创建 FrozenLake-v1 环境\n",
    "env = gym.make() # TODO 使用正确的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji_UrI5l2zzn"
   },
   "source": [
    "### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNxUbPMP0akP",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:29:15.396972Z",
     "start_time": "2025-08-09T13:29:15.383029Z"
    }
   },
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KASNViqL4tZn"
   },
   "source": [
    "你可以像这样创建自己的自定义网格：\n",
    "\n",
    "```python\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "\n",
    "但我们现在将使用默认环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### 让我们看看环境是什么样子的：\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZNPG0g_UGCfh",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:29:19.239813Z",
     "start_time": "2025-08-09T13:29:19.234778Z"
    }
   },
   "source": [
    "# 我们使用 gym.make(\"<name_of_the_environment>\") 创建我们的环境 - `is_slippery=False`：由于冰湖的非湿滑特性，智能体始终朝预定方向移动（确定性）。\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # 获取一个随机观察"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 11\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "我们看到 `Observation Space Shape Discrete(16)`，观察结果是一个整数，表示**智能体的当前位置，即 current_row * ncols + current_col（其中行和列都从 0 开始）**。\n",
    "\n",
    "例如，4x4 地图中的目标位置可以计算如下：3 * 4 + 3 = 15。可能观察的数量取决于地图的大小。**例如，4x4 地图有 16 个可能的观察。**\n",
    "\n",
    "\n",
    "例如，这是 state = 0 的样子：\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "We5WqOBGLoSm",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:29:26.383426Z",
     "start_time": "2025-08-09T13:29:26.379398Z"
    }
   },
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # 随机采取一个动作"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "动作空间（智能体可以采取的可能动作的集合）是离散的，有 4 个可用动作 🎮：\n",
    "- 0: 向左\n",
    "- 1: 向下\n",
    "- 2: 向右\n",
    "- 3: 向上\n",
    "\n",
    "奖励函数 💰:\n",
    "- 到达目标: +1\n",
    "- 到达洞: 0\n",
    "- 到达冰面: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhWblk3Awr"
   },
   "source": [
    "## 创建和初始化 Q-table 🗄️\n",
    "\n",
    "(👀 伪代码的第 1 步)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "是时候初始化我们的 Q-table 了！要知道要使用多少行（状态）和列（动作），我们需要知道动作和观察空间。我们之前已经知道了它们的值，但我们希望以编程方式获取它们，以便我们的算法可以推广到不同的环境。Gym 为我们提供了一种方法：`env.action_space.n` 和 `env.observation_space.n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3ZCdluj3k0l"
   },
   "outputs": [],
   "source": [
    "state_space =\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space =\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCddoOXM3UQH"
   },
   "outputs": [],
   "source": [
    "# 让我们创建大小为 (state_space, action_space) 的 Qtable，并使用 np.zeros 将每个值初始化为 0。np.zeros 需要一个元组 (a,b)\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable =\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YfvrqRt3jdR"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OdoKL63eDD"
   },
   "source": [
    "### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HuTKv3th3ohG",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:29:47.291238Z",
     "start_time": "2025-08-09T13:29:47.286522Z"
    }
   },
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"共有\", state_space, \"个可能的状态\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"共有\", action_space, \"个可能的动作\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 16 个可能的状态\n",
      "共有 4 个可能的动作\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lnrb_nX33fJo",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:29:49.660945Z",
     "start_time": "2025-08-09T13:29:49.656881Z"
    }
   },
   "source": [
    "# 让我们创建大小为 (state_space, action_space) 的 Qtable，并使用 np.zeros 将每个值初始化为 0\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y0WlgkVO3Jf9",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:02.624755Z",
     "start_time": "2025-08-09T13:30:02.620898Z"
    }
   },
   "source": "Qtable_frozenlake = initialize_q_table(state_space, action_space)",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## 定义贪婪策略 🤖\n",
    "\n",
    "请记住，由于 Q-Learning 是一种**离策略**算法，我们有两种策略。这意味着我们使用**不同的策略来执行动作和更新价值函数**。\n",
    "\n",
    "- Epsilon-greedy 策略（执行策略）\n",
    "- Greedy 策略（更新策略）\n",
    "\n",
    "贪婪策略也将是 Q-learning 智能体完成训练时的最终策略。贪婪策略用于使用 Q-table 选择一个动作。\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # 利用：采取具有最高状态、动作值的动作\n",
    "  action =\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_-8b8z5k54"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "se2OzWGW5kYJ",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:07.771728Z",
     "start_time": "2025-08-09T13:30:07.768246Z"
    }
   },
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # 利用：采取具有最高状态、动作值的动作\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flILKhBU3yZ7"
   },
   "source": [
    "## 定义 epsilon-greedy 策略 🤖\n",
    "\n",
    "Epsilon-greedy 是处理探索/利用权衡的训练策略。\n",
    "\n",
    "Epsilon-greedy 的思想是：\n",
    "\n",
    "- 以*概率 1-ε*：**我们进行利用**（即我们的智能体选择具有最高状态-动作对值的动作）。\n",
    "\n",
    "- 以*概率 ε*：我们进行**探索**（尝试一个随机动作）。\n",
    "\n",
    "随着训练的继续，我们逐渐**减少 epsilon 值，因为我们需要的探索越来越少，而利用越来越多。**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bj7x3in3_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # 随机生成一个 0 到 1 之间的数字\n",
    "  random_num =\n",
    "  # 如果 random_num > epsilon --> 利用\n",
    "  if random_num > epsilon:\n",
    "    # 在给定状态下采取具有最高值的动作\n",
    "    # np.argmax 在这里很有用\n",
    "    action =\n",
    "  # 否则 --> 探索\n",
    "  else:\n",
    "    action = # 采取一个随机动作\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R5ej1fS4P2V"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cYxHuckr4LiG",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:16.122482Z",
     "start_time": "2025-08-09T13:30:16.116928Z"
    }
   },
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # 随机生成一个 0 到 1 之间的数字\n",
    "  random_num = random.uniform(0,1)\n",
    "  # 如果 random_num > epsilon --> 利用\n",
    "  if random_num > epsilon:\n",
    "    # 在给定状态下采取具有最高值的动作\n",
    "    # np.argmax 在这里很有用\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # 否则 --> 探索\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## 定义超参数 ⚙️\n",
    "\n",
    "与探索相关的超参数是最重要的超参数之一。\n",
    "\n",
    "- 我们需要确保我们的智能体**探索足够的状态空间**以学习一个好的价值近似。为此，我们需要对 epsilon 进行渐进衰减。\n",
    "- 如果你过快地减少 epsilon（衰减率过高），**你将面临智能体卡住的风险**，因为你的智能体没有探索足够的状态空间，因此无法解决问题。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y1tWn0tycWZ1",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:18.487815Z",
     "start_time": "2025-08-09T13:30:18.482803Z"
    }
   },
   "source": [
    "# 训练参数\n",
    "n_training_episodes = 10000  # 总训练回合数\n",
    "learning_rate = 0.7          # 学习率\n",
    "\n",
    "# 评估参数\n",
    "n_eval_episodes = 100        # 总测试回合数\n",
    "\n",
    "# 环境参数\n",
    "env_id = \"FrozenLake-v1\"     # 环境名称\n",
    "max_steps = 99               # 每回合最大步数\n",
    "gamma = 0.95                 # 折扣率\n",
    "eval_seed = []               # 环境的评估种子\n",
    "\n",
    "# 探索参数\n",
    "max_epsilon = 1.0             # 开始时的探索概率\n",
    "min_epsilon = 0.05            # 最小探索概率\n",
    "decay_rate = 0.0005            # 探索概率的指数衰减率"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "## 创建训练循环方法\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "训练循环如下：\n",
    "\n",
    "```\n",
    "对于总训练回合中的每个回合：\n",
    "\n",
    "减少 epsilon（因为我们需要的探索越来越少）\n",
    "重置环境\n",
    "\n",
    "  对于最大时间步中的每一步：\n",
    "    使用 epsilon greedy 策略选择动作 At\n",
    "    采取动作 (a) 并观察结果状态 (s') 和奖励 (r)\n",
    "    使用贝尔曼方程更新 Q 值 Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    如果完成，则结束回合\n",
    "    我们的下一个状态是新状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paOynXy3aoJW"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # 减少 epsilon（因为我们需要的探索越来越少）\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # 重复\n",
    "    for step in range(max_steps):\n",
    "      # 使用 epsilon greedy 策略选择动作 At\n",
    "      action =\n",
    "\n",
    "      # 采取动作 At 并观察 Rt+1 和 St+1\n",
    "      # 采取动作 (a) 并观察结果状态 (s') 和奖励 (r)\n",
    "      new_state, reward, terminated, truncated, info =\n",
    "\n",
    "      # 更新 Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] =\n",
    "\n",
    "      # 如果终止或截断，则结束回合\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # 我们的下一个状态是新状态\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpk2ePoem3r"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IyZaYbUAeolw",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:25.594053Z",
     "start_time": "2025-08-09T13:30:25.587986Z"
    }
   },
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # 减少 epsilon（因为我们需要的探索越来越少）\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # 重置环境\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # 重复\n",
    "    for step in range(max_steps):\n",
    "      # 使用 epsilon greedy 策略选择动作 At\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # 采取动作 At 并观察 Rt+1 和 St+1\n",
    "      # 采取动作 (a) 并观察结果状态 (s') 和奖励 (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # 更新 Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # 如果终止或截断，则结束回合\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # 我们的下一个状态是新状态\n",
    "      state = new_state\n",
    "  return Qtable"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwKQ4tUdhGI"
   },
   "source": [
    "## 训练 Q-Learning 智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DPBxfjJdTCOH",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:30.379565Z",
     "start_time": "2025-08-09T13:30:28.758937Z"
    }
   },
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92a3e60e383749888ce840ac7e712f46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "## 让我们看看我们的 Q-Learning 表现在是什么样子 👀"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nmfchsTITw4q",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:30:32.637654Z",
     "start_time": "2025-08-09T13:30:32.632654Z"
    }
   },
   "source": [
    "Qtable_frozenlake"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUrWkxsHccXD"
   },
   "source": [
    "## 评估方法 📝\n",
    "\n",
    "- 我们定义了用于测试我们的 Q-Learning 智能体的评估方法。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jNl0_JO2cbkm",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:32:32.779855Z",
     "start_time": "2025-08-09T13:32:32.774642Z"
    }
   },
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  评估智能体 ``n_eval_episodes`` 次，并返回平均奖励和奖励的标准差。\n",
    "  :param env: 评估环境\n",
    "  :param max_steps: 每回合的最大步数\n",
    "  :param n_eval_episodes: 评估智能体的回合数\n",
    "  :param Q: Q-table\n",
    "  :param seed: 评估种子数组（用于 taxi-v3）\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # 采取在该状态下具有最大预期未来奖励的动作（索引）\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jJqjaoAnxUo"
   },
   "source": [
    "## 评估我们的 Q-Learning 智能体 📈\n",
    "\n",
    "- 通常，你应该有一个 1.0 的平均奖励\n",
    "- **环境相对容易**，因为状态空间非常小 (16)。你可以尝试[用湿滑版本替换它](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)，这会引入随机性，使环境更复杂。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fAgB7s0HEFMm",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:32:36.033518Z",
     "start_time": "2025-08-09T13:32:36.003571Z"
    }
   },
   "source": [
    "# 评估我们的智能体\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21b1a15bd6da4027b69a0cd4a183ad31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaP3bPdg1DV"
   },
   "source": [
    "## 将我们训练好的模型发布到 Hub 🔥\n",
    "\n",
    "现在我们在训练后看到了好的结果，**我们可以用一行代码将我们训练好的模型发布到 Hub 🤗**。\n",
    "\n",
    "这是一个模型卡的示例：\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Model card\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0k1JQjpMq3"
   },
   "source": [
    "在底层，Hub 使用基于 git 的存储库（如果你不知道 git 是什么，请不要担心），这意味着你可以在试验和改进智能体时使用新版本更新模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5LrR-joIHD"
   },
   "source": [
    "#### 不要修改此代码"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jex3i9lZ8ksX",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:33:24.744161Z",
     "start_time": "2025-08-09T13:33:23.515097Z"
    }
   },
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "修改: 指明编码格式"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qo57HBn3W74O",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:41:08.159233Z",
     "start_time": "2025-08-09T13:41:08.153706Z"
    }
   },
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  生成智能体的回放视频\n",
    "  :param env\n",
    "  :param Qtable: 我们智能体的 Qtable\n",
    "  :param out_directory\n",
    "  :param fps: 每秒多少帧（对于 taxi-v3 和 frozenlake-v1，我们使用 1）\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # 采取在该状态下具有最大预期未来奖励的动作（索引）\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # 我们直接将 next_state = state 用于录制逻辑\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(\n",
    "        out_directory,\n",
    "        [np.array(img) for img in images],  # 简化列表生成式（无需枚举i）\n",
    "        fps=fps,\n",
    "        codec='libx264'  # 明确指定编码格式为 H.264\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "def push_to_hub(\n",
    "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
    "):\n",
    "    \"\"\"\n",
    "    评估、生成视频并将模型上传到 Hugging Face Hub。\n",
    "    此方法执行完整的流程：\n",
    "    - 它评估模型\n",
    "    - 它生成模型卡\n",
    "    - 它生成智能体的回放视频\n",
    "    - 它将所有内容推送到 Hub\n",
    "\n",
    "    :param repo_id: Hugging Face Hub 中模型存储库的 id\n",
    "    :param env\n",
    "    :param video_fps: 录制视频回放的每秒帧数\n",
    "    （对于 taxi-v3 和 frozenlake-v1，我们使用 1）\n",
    "    :param local_repo_path: 本地存储库的位置\n",
    "    \"\"\"\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "\n",
    "    eval_env = env\n",
    "    api = HfApi()\n",
    "\n",
    "    # 第 1 步：创建存储库\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # 第 2 步：下载文件\n",
    "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
    "\n",
    "    # 第 3 步：保存模型\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
    "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "            model[\"slippery\"] = False\n",
    "\n",
    "    # 序列化模型\n",
    "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # 第 4 步：评估模型并使用评估指标构建 JSON\n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
    "    )\n",
    "\n",
    "    evaluate_data = {\n",
    "        \"env_id\": model[\"env_id\"],\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
    "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # 编写一个名为“results.json”的 JSON 文件，其中包含\n",
    "    # 评估结果\n",
    "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # 第 5 步：创建模型卡\n",
    "    env_name = model[\"env_id\"]\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
    "\n",
    "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "        env_name += \"-\" + \"no_slippery\"\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
    "\n",
    "    # 添加指标\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "    )\n",
    "\n",
    "    # 合并两个字典\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Q-Learning** 智能体玩 **{env_id}**\n",
    "  这是**Q-Learning** 智能体玩 **{env_id}** 的训练模型。\n",
    "\n",
    "  ## 用法\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
    "\n",
    "  # 不要忘记检查是否需要添加其他属性（is_slippery=False 等）\n",
    "  env = gym.make(model[\"env_id\"])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
    "\n",
    "    readme_path = repo_local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    print(readme_path.exists())\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # 将我们的指标保存到 Readme 元数据\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # 第 6 步：录制视频\n",
    "    video_path = repo_local_path / \"replay.mp4\"\n",
    "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
    "\n",
    "    # 第 7 步。将所有内容推送到 Hub\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=repo_local_path,\n",
    "        path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(\"你的模型已推送到 Hub。你可以在此处查看你的模型：\", repo_url)"
   ],
   "metadata": {
    "id": "U4mdUTKkGnUd",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:41:18.452710Z",
     "start_time": "2025-08-09T13:41:18.438206Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J6cet_ogSS"
   },
   "source": [
    "### .\n",
    "\n",
    "通过使用 `push_to_hub`，你**评估、录制回放、生成智能体的模型卡并将其推送到 Hub**。\n",
    "\n",
    "这样：\n",
    "- 你可以**展示我们的工作** 🔥\n",
    "- 你可以**可视化你的智能体玩游戏** 👀\n",
    "- 你可以**与社区共享一个智能体，其他人可以使用** 💾\n",
    "- 你可以**访问排行榜 🏆，查看你的智能体与同学相比的表现** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "为了能够与社区分享你的模型，还需要执行三个步骤：\n",
    "\n",
    "1️⃣（如果尚未完成）创建一个 HF 帐户 ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ 登录，然后，你需要从 Hugging Face 网站存储你的身份验证令牌。\n",
    "- 创建一个新令牌（https://huggingface.co/settings/tokens）**具有写入角色**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QB5nIcxR8paT",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:42:57.035404Z",
     "start_time": "2025-08-09T13:42:57.023355Z"
    }
   },
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b98956ae3f744619c57740c94b9d888"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "如果你不想使用 Google Colab 或 Jupyter Notebook，则需要改用此命令：`huggingface-cli login`（或 `login`）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc5AfUeFo3xH"
   },
   "source": [
    "3️⃣ 我们现在准备好使用 `push_to_hub()` 函数将我们训练好的智能体推送到 🤗 Hub 🔥\n",
    "\n",
    "- 让我们创建**包含超参数和 Q_table 的模型字典**。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FiMqxqVHg0I4",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:43:23.899169Z",
     "start_time": "2025-08-09T13:43:23.895268Z"
    }
   },
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_frozenlake\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kld-AEso3xH"
   },
   "source": [
    "让我们填写 `push_to_hub` 函数：\n",
    "\n",
    "- `repo_id`：将要创建/更新的 Hugging Face Hub 存储库的名称 `\n",
    "(repo_id = {username}/{repo_name})`\n",
    "💡 一个好的 `repo_id` 是 `{username}/q-{env_id}`\n",
    "- `model`：我们的模型字典，包含超参数和 Qtable。\n",
    "- `env`：环境。\n",
    "- `commit_message`：提交的消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBo2umnXpPd"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RpOTtSt83kPZ",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:43:55.455518Z",
     "start_time": "2025-08-09T13:43:27.480008Z"
    }
   },
   "source": [
    "username = \"a1024053774\" # 填写此项\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6ac573bbb0d48acbc735e2d8fc16fae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2a5fb2e8bc04070905511fb50480472"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9e778645dcf4515bd33bece3f7565ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "q-learning.pkl:   0%|          | 0.00/915 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f4314837f334abba182c5a530995b98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2875IGsprzq"
   },
   "source": [
    "恭喜 🥳 你刚刚从头开始实现、训练和上传了你的第一个强化学习智能体。\n",
    "FrozenLake-v1 no_slippery 是一个非常简单的环境，让我们尝试一个更难的 🔥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lN8Bz7yvLt"
   },
   "source": [
    "# 第 2 部分：出租车-v3 🚖\n",
    "\n",
    "## 创建和了解 [Taxi-v3 🚕](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "---\n",
    "\n",
    "💡 开始使用环境时，一个好习惯是查看其文档\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "\n",
    "在 `Taxi-v3` 🚕 中，网格世界中有四个指定位置，分别用 R（红色）、G（绿色）、Y（黄色）和 B（蓝色）表示。\n",
    "\n",
    "当回合开始时，**出租车从一个随机方块开始**，乘客在一个随机位置。出租车开到乘客的位置，**接上乘客**，开到乘客的目的地（四个指定位置中的另一个），然后**让乘客下车**。乘客下车后，回合结束。\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gL0wpeO8gpej",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:49:11.450981Z",
     "start_time": "2025-08-09T13:49:11.440566Z"
    }
   },
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "有 **500 个离散状态，因为有 25 个出租车位置，5 个可能的乘客位置**（包括乘客在出租车内的情况）和 **4 个目的地位置。**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_TPNaGSZrgqA",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:50:02.192117Z",
     "start_time": "2025-08-09T13:50:02.187251Z"
    }
   },
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"共有\", state_space, \"个可能的状态\")"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CdeeZuokrhit",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:50:08.018770Z",
     "start_time": "2025-08-09T13:50:08.013259Z"
    }
   },
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"共有\", action_space, \"个可能的动作\")\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "动作空间（智能体可以采取的可能动作的集合）是离散的，有 **6 个可用动作 🎮**：\n",
    "\n",
    "- 0: 向南移动\n",
    "- 1: 向北移动\n",
    "- 2: 向东移动\n",
    "- 3: 向西移动\n",
    "- 4: 接上乘客\n",
    "- 5: 让乘客下车\n",
    "\n",
    "奖励函数 💰:\n",
    "\n",
    "- 每走一步扣 1 分，除非触发其他奖励。\n",
    "- 运送乘客得 20 分。\n",
    "- 非法执行“接上”和“让下”动作扣 10 分。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "US3yDXnEtY9I",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:50:13.321792Z",
     "start_time": "2025-08-09T13:50:13.316336Z"
    }
   },
   "source": [
    "# 创建我们的 Q 表，包含 state_size 行和 action_size 列 (500x6)\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi .shape)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUMKPH0_LJyH"
   },
   "source": [
    "## 定义超参数 ⚙️\n",
    "\n",
    "⚠ 不要修改 EVAL_SEED：eval_seed 数组**允许我们用相同的出租车起始位置评估每个同学的智能体**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AB6n__hhg7YS",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:50:19.386517Z",
     "start_time": "2025-08-09T13:50:19.379786Z"
    }
   },
   "source": [
    "# 训练参数\n",
    "n_training_episodes = 25000   # 总训练回合数\n",
    "learning_rate = 0.7           # 学习率\n",
    "\n",
    "# 评估参数\n",
    "n_eval_episodes = 100        # 总测试回合数\n",
    "\n",
    "# 不要修改 EVAL_SEED\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # 评估种子，这确保了所有同学的智能体都在相同的出租车起始位置上进行训练\n",
    "                                                          # 每个种子都有一个特定的起始状态\n",
    "\n",
    "# 环境参数\n",
    "env_id = \"Taxi-v3\"           # 环境名称\n",
    "max_steps = 99               # 每回合最大步数\n",
    "gamma = 0.95                 # 折扣率\n",
    "\n",
    "# 探索参数\n",
    "max_epsilon = 1.0             # 开始时的探索概率\n",
    "min_epsilon = 0.05           # 最小探索概率\n",
    "decay_rate = 0.005            # 探索概率的指数衰减率\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMORo1VLTsX"
   },
   "source": [
    "## 训练我们的 Q-Learning 智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WwP3Y2z2eS-K",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:51:08.710740Z",
     "start_time": "2025-08-09T13:50:57.122620Z"
    }
   },
   "source": "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b9536aa76254f1ead06da0b898d5540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T13:52:56.153041Z",
     "start_time": "2025-08-09T13:52:56.147651Z"
    }
   },
   "cell_type": "code",
   "source": "Qtable_taxi",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 2.75200369,  3.94947757,  2.75200369,  3.94947757,  5.20997639,\n",
       "        -5.05052243],\n",
       "       [ 7.93349184,  9.40367562,  7.93349184,  9.40367562, 10.9512375 ,\n",
       "         0.40367562],\n",
       "       ...,\n",
       "       [10.9512375 , 12.58025   , 10.9512375 ,  9.40367562,  1.9512375 ,\n",
       "         1.9512375 ],\n",
       "       [ 4.92908749,  6.53681725, -3.74332345,  5.56660966, -5.90377538,\n",
       "        -4.02979836],\n",
       "       [16.09996805, 14.291498  , 16.09965603, 18.        ,  7.0999992 ,\n",
       "         7.09999716]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdu0SueLVl2"
   },
   "source": [
    "## 创建一个模型字典 💾 并将我们训练好的模型发布到 Hub 🔥\n",
    "\n",
    "- 我们创建一个模型字典，其中包含所有训练超参数以实现可重复性，以及 Q-Table。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0a1FpE_3hNYr",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:53:01.568332Z",
     "start_time": "2025-08-09T13:53:01.563443Z"
    }
   },
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_taxi\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dhQtiQozhOn1",
    "ExecuteTime": {
     "end_time": "2025-08-09T13:54:50.349496Z",
     "start_time": "2025-08-09T13:54:03.023965Z"
    }
   },
   "source": [
    "username = \"a1024053774\" # 填写此项\n",
    "repo_name = \"taxi-v3\" # 填写此项\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad297da08da9447e9fce674a79c5880f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48e7379e0753415bbe09ca9a98cd3207"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d33b6f3add2425ba088469b88d5d7dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb820ac511b748a7a06c34790ac7c1a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "replay.mp4:   0%|          | 0.00/125k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14bdba4a2cf14c8ab97e6b02bb7e2313"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "q-learning.pkl:   0%|          | 0.00/24.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aca36a81278f4391af987a88ed36a8ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b91e739740a4bfb9cb1f05ffddf79f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgSdjgbIpRti"
   },
   "source": [
    "现在它在 Hub 上，你可以使用排行榜 🏆 将你的 Taxi-v3 的结果与同学进行比较 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgIO70c0bu2"
   },
   "source": [
    "# 第 3 部分：从 Hub 加载 🔽\n",
    "\n",
    "Hugging Face Hub 🤗 的惊人之处在于，你可以轻松地从社区加载强大的模型。\n",
    "\n",
    "从 Hub 加载保存的模型非常简单：\n",
    "\n",
    "1. 你可以访问 https://huggingface.co/models?other=q-learning 查看所有 q-learning 保存模型的列表。\n",
    "2. 你选择一个并复制其 repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTth6thRoC6X"
   },
   "source": [
    "3. 然后我们只需要使用 `load_from_hub`：\n",
    "- The repo_id\n",
    "- The filename: the saved model inside the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtrfoTaBoNrd"
   },
   "source": [
    "#### 不要修改此代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo8qEzNtCaVI"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    从 Hugging Face Hub 下载模型。\n",
    "    :param repo_id: Hugging Face Hub 中模型存储库的 id\n",
    "    :param filename: 存储库中模型 zip 文件的名称\n",
    "    \"\"\"\n",
    "    # 从 Hub 获取模型，下载模型并将其缓存在本地磁盘上\n",
    "    pickle_model = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename\n",
    "    )\n",
    "\n",
    "    with open(pickle_model, 'rb') as f:\n",
    "      downloaded_model_file = pickle.load(f)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_sM2gNioPZH"
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUm9lz2gCQcU"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # 尝试使用另一个模型\n",
    "\n",
    "print(model)\n",
    "env = gym.make(model[\"env_id\"])\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7pL8rg1MulN"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # 尝试使用另一个模型\n",
    "\n",
    "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## 一些额外的挑战 🏆\n",
    "\n",
    "学习的最好方法是**自己尝试**！正如你所看到的，当前的智能体表现不佳。作为第一个建议，你可以训练更多步。我们看到，经过 1,000,000 步的训练，结果非常出色！\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)中，你会找到你的智能体。你能登上榜首吗？\n",
    "\n",
    "以下是一些登上排行榜的建议：\n",
    "\n",
    "* 训练更多步\n",
    "* 通过查看同学的做法，尝试不同的超参数。\n",
    "* **将你新训练的模型**推送到 Hub 🔥\n",
    "\n",
    "在冰上行走和开出租车对你来说太无聊了吗？尝试**更改环境**，为什么不使用 FrozenLake-v1 湿滑版本呢？使用 [gymnasium 文档](https://gymnasium.farama.org/)查看它们的工作原理，玩得开心 🎉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-fW-EU5WejJ"
   },
   "source": [
    "_____________________________________________________________________\n",
    "恭喜 🥳，你刚刚实现、训练和上传了你的第一个强化学习智能体。\n",
    "\n",
    "理解 Q-Learning 是**理解基于价值的方法的重要一步。**\n",
    "\n",
    "在下一个关于深度 Q-Learning 的单元中，我们会看到，虽然创建和更新 Q-table 是一个很好的策略，但**它不可扩展。**\n",
    "\n",
    "例如，想象一下你创建了一个学习玩 Doom 的智能体。\n",
    "\n",
    "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
    "\n",
    "Doom 是一个具有巨大状态空间（数百万个不同状态）的大型环境。为该环境创建和更新 Q-table 效率不高。\n",
    "\n",
    "这就是为什么我们将在下一个单元中学习深度 Q-Learning，这是一种算法，**我们使用神经网络来近似，在给定状态下，每个动作的不同 Q 值。**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "第 3 单元见！🔥\n",
    "\n",
    "## 继续学习，保持出色 🤗"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
