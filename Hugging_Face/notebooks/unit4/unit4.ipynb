{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjRWziAVU2lZ"
   },
   "source": [
    "# 第 4 单元：使用 PyTorch 编写你的第一个深度强化学习算法：Reinforce，并测试其鲁棒性 💪\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\"/>\n",
    "\n",
    "\n",
    "在本 Notebook 中，你将从头开始编写你的第一个深度强化学习算法：Reinforce (也称为蒙特卡洛策略梯度)。\n",
    "\n",
    "Reinforce 是一种*基于策略的方法*：一种**直接尝试优化策略而不使用动作价值函数**的深度强化学习算法。\n",
    "\n",
    "更准确地说，Reinforce 是一种*策略梯度方法*，它是*基于策略的方法*的一个子类，旨在使用**梯度上升法估计最优策略的权重来直接优化策略**。\n",
    "\n",
    "为了测试其鲁棒性，我们将在 2 个不同的简单环境中训练它：\n",
    "- Cartpole-v1\n",
    "- PixelcopterEnv\n",
    "\n",
    "⬇️ 这是**你将在本 Notebook 结束时实现**的效果示例。⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Environments\"/>\n"
   ],
   "metadata": {
    "id": "s4rBom2sbo7S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🎮 环境:\n",
    "\n",
    "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
    "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "### 📚 RL 库:\n",
    "\n",
    "- Python\n",
    "- PyTorch\n",
    "\n",
    "\n",
    "我们一直在努力改进我们的教程，因此**如果你在本 Notebook 中发现任何问题**，请在[GitHub 仓库中提出问题](https://github.com/huggingface/deep-rl-class/issues)。"
   ],
   "metadata": {
    "id": "BPLwsPajb1f8"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_WSo0VUV99t"
   },
   "source": [
    "## 本 Notebook 的目标 🏆\n",
    "在本 Notebook 结束时，你将能够：\n",
    "- **使用 PyTorch 从零开始编写 Reinforce 算法。**\n",
    "- **使用简单的环境测试你的智能体的鲁棒性。**\n",
    "- **将你训练好的智能体推送到 Hub**，并附上精彩的回放视频和评估分数 🔥。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPrZg2eWa4R"
   },
   "source": [
    "## 本 Notebook 来自深度强化学习课程\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "在这个免费课程中，你将：\n",
    "\n",
    "- 📖 **在理论和实践中**学习深度强化学习。\n",
    "- 🧑‍💻 学习**使用著名的深度 RL 库**，如 Stable Baselines3、RL Baselines3 Zoo、CleanRL 和 Sample Factory 2.0。\n",
    "- 🤖 在**独特的环境中训练智能体**\n",
    "\n",
    "更多内容请查看 📚 课程大纲 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "别忘了**<a href=\"http://eepurl.com/ic5ZUD\">注册课程</a>** (我们收集你的电子邮件是为了**在每个单元发布时向你发送链接，并为你提供有关挑战和更新的信息**)。\n",
    "\n",
    "\n",
    "保持联系的最佳方式是加入我们的 Discord 服务器，与社区和我们交流 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjY-eq3eWh9O"
   },
   "source": [
    "## 先决条件 🏗️\n",
    "在深入学习本 Notebook 之前，你需要：\n",
    "\n",
    "🔲 📚 [通过阅读第 4 单元来学习策略梯度](https://huggingface.co/deep-rl-course/unit4/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 让我们从头开始编写 Reinforce 算法 🔥\n",
    "\n",
    "\n",
    "要通过认证流程验证此实践项目，你需要将训练好的模型推送到 Hub。\n",
    "\n",
    "- `Cartpole-v1` 的结果需 >= 350。\n",
    "- `PixelCopter` 的结果需 >= 5。\n",
    "\n",
    "要找到你的结果，请转到排行榜并找到你的模型，**结果 = mean_reward - std of reward**。**如果你在排行榜上看不到你的模型，请转到排行榜页面底部，然后单击刷新按钮**。\n",
    "\n",
    "有关认证流程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n"
   ],
   "metadata": {
    "id": "Bsh4ZAamchSl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 检查 GPU 支持 💪\n",
    "为了**加速智能体的训练，我们将使用 GPU**。下面的代码将检查 PyTorch 是否可以访问 CUDA enabled 的 GPU。\n",
    "如果你的计算机上有正确配置的 NVIDIA GPU，你应该会看到 `device:cuda:0`。"
   ],
   "metadata": {
    "id": "PU4FVzaoM6fC"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kaJu5FeZxXGY",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:30.819257Z",
     "start_time": "2025-08-12T18:31:30.815255Z"
    }
   },
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建一个虚拟显示器 🖥\n",
    "\n",
    "在本地运行时，如果你有桌面环境，通常不需要这一步。但是，如果你在没有显示器的服务器上运行，或者为了确保代码的可移植性，创建一个虚拟屏幕是很有用的。\n",
    "下面的单元格将安装所需的库并启动一个虚拟屏幕，这样我们就可以在后台渲染环境并录制视频了。\n"
   ],
   "metadata": {
    "id": "bTpYcVZVMzUI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 在本地安装依赖项\n",
    "要运行此 Notebook，你需要安装几个 Python 包。在你的终端中运行以下命令来安装它们。\n",
    "\n",
    "对于视频录制功能，你可能还需要安装 `ffmpeg`。在 Ubuntu/Debian 上，你可以使用 `sudo apt install ffmpeg`。在 macOS 上，可以使用 `brew install ffmpeg`。\n",
    "\n",
    "```bash\n",
    "# 安装 Python 包\n",
    "pip install gym==0.21.0 gym-games==0.2.0 pygame==2.1.0 huggingface-hub==0.8.1 protobuf==3.19.5 imageio==2.9.0 pyvirtualdisplay\n",
    "```\n",
    "下面的代码单元格是为了在 Notebook 环境中运行而保留的，但建议在终端中进行安装。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !apt install python-opengl # 在某些系统上可能需要\n",
    "# !apt install ffmpeg\n",
    "# !apt install xvfb\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 虚拟显示\n",
    "# 如果你在没有显示器的服务器上运行，或者在本地遇到渲染问题，请取消注释以下代码\n",
    "try:\n",
    "    from pyvirtualdisplay import Display\n",
    "    virtual_display = Display(visible=0, size=(1400, 900))\n",
    "    virtual_display.start()\n",
    "except ImportError:\n",
    "    print(\"PyVirtualDisplay 未安装，将跳过虚拟显示设置。在本地桌面上通常不需要它。\")\n"
   ],
   "metadata": {
    "id": "Sr-Nuyb1dBm0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjrLfPFIW8XK"
   },
   "source": [
    "## 安装依赖项 🔽\n",
    "第一步是安装依赖项。我们将安装多个库：\n",
    "\n",
    "- `gym`\n",
    "- `gym-games`: 使用 PyGame 制作的额外 gym 环境。\n",
    "- `huggingface_hub`: 🤗 作为一个中心枢纽，任何人都可以在这里共享和探索模型和数据集。它具有版本控制、指标、可视化等功能，可让你轻松与他人协作。\n",
    "\n",
    "你可能想知道我们为什么安装 `gym` 而不是 `gymnasium` (gym 的一个更新版本)？**因为我们正在使用的 `gym-games` 尚未更新以支持 `gymnasium`**。\n",
    "\n",
    "你在这里会遇到的区别：\n",
    "- 在 `gym` 中，我们没有 `terminated` 和 `truncated`，只有 `done`。\n",
    "- 在 `gym` 中，使用 `env.step()` 返回 `state, reward, done, info`\n",
    "\n",
    "你可以在这里了解更多关于 Gym 和 Gymnasium 之间的区别 👉 https://gymnasium.farama.org/content/migration-guide/\n",
    "\n",
    "\n",
    "你可以在这里看到所有可用的 Reinforce 模型 👉 https://huggingface.co/models?other=reinforce\n",
    "\n",
    "你可以在这里找到所有深度强化学习模型 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 我们从 GitHub 加载 requirements.txt 以确保版本一致\n",
    "# 建议在终端中使用 'pip install -r <url>' 或下载文件后 'pip install -r requirements-unit4.txt' 来安装\n",
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ],
   "metadata": {
    "id": "e8ZVi-uydpgL",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:32:46.601444Z",
     "start_time": "2025-08-12T12:31:50.636814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n",
      "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to c:\\users\\10240\\appdata\\local\\temp\\pip-req-build-8_9wvlyi\n",
      "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n",
      "  Cloning https://github.com/simoninithomas/gym-games to c:\\users\\10240\\appdata\\local\\temp\\pip-req-build-m8_yi_s4\n",
      "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.29.1)\n",
      "Requirement already satisfied: imageio-ffmpeg in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4)) (0.6.0)\n",
      "Collecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5))\n",
      "  Using cached PyYAML-6.0-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: gym>=0.13.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (75.8.2)\n",
      "Requirement already satisfied: pygame>=1.9.6 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2025.1.31)\n",
      "Using cached PyYAML-6.0-cp311-cp311-win_amd64.whl (143 kB)\n",
      "Installing collected packages: pyyaml\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "Successfully installed pyyaml-6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git 'C:\\Users\\10240\\AppData\\Local\\Temp\\pip-req-build-8_9wvlyi'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games 'C:\\Users\\10240\\AppData\\Local\\Temp\\pip-req-build-m8_yi_s4'\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\~aml'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.1 which is incompatible.\n",
      "langchain-community 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.1 which is incompatible.\n",
      "langchain-core 0.3.40 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "rapidocr-onnxruntime 1.3.24 requires numpy<2.0.0,>=1.19.5, but you have numpy 2.0.1 which is incompatible.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHAq6RZW3rn"
   },
   "source": [
    "## 导入包 📦\n",
    "除了导入已安装的库之外，我们还导入：\n",
    "\n",
    "- `imageio`: 一个帮助我们生成回放视频的库\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V8oadoJSWp7C",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:35.524640Z",
     "start_time": "2025-08-12T18:31:35.515796Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # 登录我们的 Hugging Face 帐户以上传模型到 Hub\n",
    "import imageio"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPecCtBL_pZ"
   },
   "source": [
    "我们现在准备好实现我们的 Reinforce 算法了 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KEyKYo2ZSC-"
   },
   "source": [
    "# 第一个智能体：玩 CartPole-v1 🤖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLArKURMyuF"
   },
   "source": [
    "## 创建 CartPole 环境并了解其工作原理\n",
    "### [环境 🎮](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH_TaLKFXo_8"
   },
   "source": [
    "### 为什么我们使用像 CartPole-v1 这样的简单环境？\n",
    "正如[强化学习技巧和窍门](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)中所解释的，当你从头开始实现你的智能体时，你需要**在进入更深层次之前，确保它在简单的环境中正常工作并找到错误**。因为在简单的环境中找到错误会容易得多。\n",
    "\n",
    "\n",
    "> 尝试在玩具问题上获得一些“生命迹象”\n",
    "\n",
    "\n",
    "> 通过在越来越难的环境中运行来验证实现（你可以将结果与 RL zoo 进行比较）。你通常需要为此步骤运行超参数优化。\n",
    "___\n",
    "### CartPole-v1 环境\n",
    "\n",
    "> 一根杆通过一个无驱动的关节连接到一个在无摩擦轨道上移动的推车上。摆锤垂直放置在推车上，目标是通过向推车施加向左和向右的力来平衡杆。\n",
    "\n",
    "\n",
    "\n",
    "所以，我们从 CartPole-v1 开始。目标是向左或向右推动推车，**以便杆保持平衡。**\n",
    "\n",
    "如果出现以下情况，回合结束：\n",
    "- 杆的角度大于 ±12°\n",
    "- 推车位置大于 ±2.4\n",
    "- 回合长度大于 500\n",
    "\n",
    "每当杆保持平衡，我们每个时间步都会获得 +1 的奖励 💰。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "POOOk15_K6KA",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:39.894378Z",
     "start_time": "2025-08-12T18:31:39.888666Z"
    }
   },
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# 创建环境\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# 创建评估环境\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# 获取状态空间和动作空间\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FMLFrjiBNLYJ",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:46.861570Z",
     "start_time": "2025-08-12T18:31:46.855363Z"
    }
   },
   "source": [
    "print(\"_____观察空间_____ \\n\")\n",
    "print(\"状态空间大小为: \", s_size)\n",
    "print(\"观察样本\", env.observation_space.sample()) # 获取一个随机观察"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____观察空间_____ \n",
      "\n",
      "状态空间大小为:  4\n",
      "观察样本 [ 1.1961311e+00 -4.3045794e+37  3.1573388e-01 -1.8938621e+38]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lu6t4sRNNWkN",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:34:06.653736Z",
     "start_time": "2025-08-12T12:34:06.646717Z"
    }
   },
   "source": [
    "print(\"\\n _____动作空间_____ \\n\")\n",
    "print(\"动作空间大小为: \", a_size)\n",
    "print(\"动作空间样本\", env.action_space.sample()) # 获取一个随机动作"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____动作空间_____ \n",
      "\n",
      "动作空间大小为:  2\n",
      "动作空间样本 1\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJMJj3WaFOz"
   },
   "source": [
    "## 让我们构建 Reinforce 架构\n",
    "这个实现基于两个实现：\n",
    "- [PyTorch 官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
    "- [Chris1nexus 改进的集成](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49kogtxBODX8"
   },
   "source": [
    "所以我们想要：\n",
    "- 两个全连接层 (fc1 和 fc2)。\n",
    "- 使用 ReLU 作为 fc1 的激活函数\n",
    "- 使用 Softmax 输出一个关于动作的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w2LHcHhVZvPZ",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:51.063260Z",
     "start_time": "2025-08-12T18:31:51.057136Z"
    }
   },
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播\n",
    "        # 状态进入 fc1 然后我们应用 ReLU 激活函数\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1 的输出进入 fc2\n",
    "        x = self.fc2(x)\n",
    "        # 我们输出 softmax\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        给定一个状态，采取行动\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m) # <--- 这里有个错误!\n",
    "        return action.item(), m.log_prob(action)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTGWL4g2eM5B"
   },
   "source": [
    "我犯了一个错误，你能猜到在哪里吗？\n",
    "\n",
    "- 为了找出答案，让我们进行一次前向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwnqGBCNePor"
   },
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "try:\n",
    "    debug_policy.act(env.reset())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14UYkoxCPaor"
   },
   "source": [
    "- 这里我们看到错误提示 `ValueError: The value argument to log_prob must be a Tensor`\n",
    "\n",
    "- 这意味着 `m.log_prob(action)` 中的 `action` 必须是一个张量 **但它不是**。\n",
    "\n",
    "- 你知道为什么吗？检查 act 函数，试着看看为什么它不起作用。\n",
    "\n",
    "提示 💡：这个实现中有些东西是错的。记住，`act` 函数**我们想要从动作的概率分布中采样一个动作**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfGJNZBUP7Vn"
   },
   "source": [
    "### (真正的) 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ho_UHf49N9i4",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:31:54.630364Z",
     "start_time": "2025-08-12T18:31:54.624445Z"
    }
   },
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample() # <-- 正确的方法是采样!\n",
    "        return action.item(), m.log_prob(action)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJWQFU_eUYw"
   },
   "source": [
    "通过使用 CartPole，调试变得更容易，因为**我们知道错误来自我们的集成，而不是来自我们的简单环境**。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 因为**我们想从动作的概率分布中采样一个动作**，我们不能使用 `action = np.argmax(m)`，因为它总是会输出概率最高的动作。\n",
    "\n",
    "- 我们需要用 `action = m.sample()` 来替换，它将从概率分布 P(.|s) 中采样一个动作"
   ],
   "metadata": {
    "id": "c-20i7Pk0l1T"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MXoqetzfIoW"
   },
   "source": [
    "### 让我们构建 Reinforce 训练算法\n",
    "这是 Reinforce 算法的伪代码：\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 当我们计算回报 Gt (伪代码第 6 行) 时，我们看到我们计算的是**从时间步 t 开始**的折扣奖励总和。\n",
    "\n",
    "- 为什么？因为我们的策略应该只**根据后果来强化动作**：所以在采取一个动作之前获得的奖励是无用的（因为它们不是由该动作引起的），**只有在动作之后发生的奖励才重要**。\n",
    "\n",
    "- 在编写代码之前，你应该阅读这一部分 [不要让过去分散你的注意力](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you)，它解释了我们为什么使用 “未来奖励”(reward-to-go) 策略梯度。\n",
    "\n",
    "我们使用一种由 [Chris1nexus](https://github.com/Chris1nexus) 编写的有趣技术来**高效地计算每个时间步的回报**。注释解释了该过程。也请随时[查看 PR 的解释](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "但总的来说，这个想法是**高效地计算每个时间步的回报**。"
   ],
   "metadata": {
    "id": "QmcXG-9i2Qu2"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O554nUGPpcoq"
   },
   "source": [
    "你可能会问的第二个问题是**为什么我们要最小化损失**？你说的是梯度上升而不是梯度下降？\n",
    "\n",
    "- 我们想要最大化我们的效用函数 $J(\\theta)$，但在 PyTorch 中，就像在 Tensorflow 中一样，最好是**最小化一个目标函数。**\n",
    "    - 所以，假设我们想在某个时间步强化动作 3。训练前，这个动作的概率 P 是 0.25。\n",
    "    - 所以我们想要修改 $\\theta$ 使得 $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - 因为所有的概率 P 之和必须为 1，最大化 $\\pi_\\theta(a_3|s; \\theta)$ 将**最小化其他动作的概率。**\n",
    "    - 所以我们应该告诉 PyTorch **最小化 $1 - \\pi_\\theta(a_3|s; \\theta)$。**\n",
    "    - 当 $\\pi_\\theta(a_3|s; \\theta)$ 接近 1 时，这个损失函数趋近于 0。\n",
    "    - 所以我们是在鼓励梯度去最大化 $\\pi_\\theta(a_3|s; \\theta)$。\n",
    "\n",
    "在我们的实现中，我们不最小化 $1 - \\pi_\\theta(a_t|s_t)$，而是最小化 $-\\log \\pi_\\theta(a_t|s_t) * G_t$。这在效果上是相同的：我们想增加一个动作的对数概率，而这个动作会带来高的回报。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iOdv8Q9NfLK7",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:34:15.832305Z",
     "start_time": "2025-08-12T12:34:15.821236Z"
    }
   },
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们在训练期间计算分数\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第 3 行\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset() # TODO: 重置环境\n",
    "        # 伪代码第 4 行\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state) # TODO: 获取动作\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) # TODO: 在环境中执行一步\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第 6 行：计算回报\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        \n",
    "        # 从后向前计算每个时间步的折扣回报\n",
    "        # G_t = r_t + gamma * G_{t+1}\n",
    "        # G_{T-1} = r_{T-1} (其中 T 是回合的最后一步)\n",
    "        # 这种方法（动态规划）可以有效地重用计算过的未来回报\n",
    "        # 避免了 O(N^2) 的朴素计算\n",
    "        \n",
    "        # 我们从最后一个时间步开始向第一个时间步计算，\n",
    "        # 以便利用上面介绍的公式，避免不必要的重复计算。\n",
    "        # 因此，队列 `returns` 将按时间顺序存储回报，从 t=0 到 t=n_steps\n",
    "        # 这要归功于 appendleft() 函数，它允许以 O(1) 的常数时间在位置 0 处追加\n",
    "        # 而普通的 python 列表则需要 O(N) 的时间来完成此操作。\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(rewards[t] + gamma * disc_return_t) # TODO: 在这里完成\n",
    "\n",
    "        ## 标准化回报可以使训练更稳定\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        ## eps 是最小的可表示浮点数，\n",
    "        # 将其添加到回报的标准差中以避免数值不稳定\n",
    "        returns = torch.tensor(list(returns))\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # 伪代码第 7 行：计算策略损失\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第 8 行：PyTorch 偏好梯度下降\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('回合 {}\\t平均分数: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB0Cxrw1StrP"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NCNvyElRStWG",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:32:02.624063Z",
     "start_time": "2025-08-12T18:32:02.615605Z"
    }
   },
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们在训练期间计算分数\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第 3 行\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # 伪代码第 4 行\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第 6 行：计算回报\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        \n",
    "        # 从后向前计算每个时间步的折扣回报\n",
    "        # G_t = r_t + gamma * G_{t+1}\n",
    "        # G_{T-1} = r_{T-1} (其中 T 是回合的最后一步)\n",
    "        # 这种方法（动态规划）可以有效地重用计算过的未来回报\n",
    "        # 避免了 O(N^2) 的朴素计算\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(rewards[t] + gamma * disc_return_t)\n",
    "\n",
    "        ## 标准化回报可以使训练更稳定\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps 是最小的可表示浮点数，\n",
    "        # 将其添加到回报的标准差中以避免数值不稳定\n",
    "        returns = torch.tensor(list(returns))\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # 伪代码第 7 行：计算策略损失\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第 8 行：PyTorch 偏好梯度下降\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('回合 {}\\t平均分数: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWhQyJjfpEt"
   },
   "source": [
    "## 训练它\n",
    "- 我们现在准备好训练我们的智能体了。\n",
    "- 但首先，我们定义一个包含所有训练超参数的变量。\n",
    "- 你可以（也应该 😉）更改训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utRe1NgtVBYF"
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16, # 隐藏层大小\n",
    "    \"n_training_episodes\": 1000, # 训练回合数\n",
    "    \"n_evaluation_episodes\": 10, # 评估回合数\n",
    "    \"max_t\": 1000, # 每个回合的最大步数\n",
    "    \"gamma\": 1.0, # 折扣因子\n",
    "    \"lr\": 1e-2, # 学习率\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3lWyVXBVfl6"
   },
   "outputs": [],
   "source": [
    "# 创建策略并将其放置到设备上\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGf-hQCnfouB"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"],\n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qajj2kXqhB3g"
   },
   "source": [
    "## 定义评估方法 📝\n",
    "- 在这里，我们定义将用于测试我们的 Reinforce 智能体的评估方法。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3FamHmxyhBEU",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:37:39.664220Z",
     "start_time": "2025-08-12T18:37:39.657718Z"
    }
   },
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  在 n_eval_episodes 个回合中评估智能体，并返回平均奖励和奖励的标准差。\n",
    "  :param env: 评估环境\n",
    "  :param n_eval_episodes: 评估智能体的回合数\n",
    "  :param policy: Reinforce 智能体\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdH2QCrLTrlT"
   },
   "source": [
    "## 评估我们的智能体 📈"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ohGSXDyHh0xx",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:33:09.119339Z",
     "start_time": "2025-08-12T18:33:09.081784Z"
    }
   },
   "source": [
    "evaluate_agent(eval_env,\n",
    "               cartpole_hyperparameters[\"max_t\"],\n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cartpole_hyperparameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m evaluate_agent(eval_env,\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m                cartpole_hyperparameters[\u001B[33m\"\u001B[39m\u001B[33mmax_t\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      3\u001B[39m                cartpole_hyperparameters[\u001B[33m\"\u001B[39m\u001B[33mn_evaluation_episodes\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      4\u001B[39m                cartpole_policy)\n",
      "\u001B[31mNameError\u001B[39m: name 'cartpole_hyperparameters' is not defined"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CoeLkQ7TpO8"
   },
   "source": [
    "### 在 Hub 上发布我们训练好的模型 🔥\n",
    "现在我们看到训练后取得了不错的结果，我们可以用一行代码将我们训练好的模型发布到 Hub 上 🤗。\n",
    "\n",
    "这是一个模型卡的示例：\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmhs1k-cftIq"
   },
   "source": [
    "### 推送到 Hub\n",
    "#### 不要修改此代码"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os"
   ],
   "metadata": {
    "id": "LIVsvlW_8tcw",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:33:16.710690Z",
     "start_time": "2025-08-12T18:33:16.705800Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lo4JH45if81z",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:33:18.537400Z",
     "start_time": "2025-08-12T18:33:18.531729Z"
    }
   },
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  生成智能体的回放视频\n",
    "  :param env\n",
    "  :param policy: 我们智能体的策略\n",
    "  :param out_directory\n",
    "  :param fps: 每秒帧数\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # 根据给定状态采取具有最大预期未来奖励的动作\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # 为了录制逻辑，我们直接将 next_state = state\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "source": [
    "def push_to_hub(repo_id,\n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  评估、生成视频并上传模型到 Hugging Face Hub。\n",
    "  此方法完成整个流程：\n",
    "  - 评估模型\n",
    "  - 生成模型卡\n",
    "  - 生成智能体的回放视频\n",
    "  - 将所有内容推送到 Hub\n",
    "\n",
    "  :param repo_id: Hugging Face Hub 上的模型仓库 ID\n",
    "  :param model: 我们想要保存的 PyTorch 模型\n",
    "  :param hyperparameters: 训练超参数\n",
    "  :param eval_env: 评估环境\n",
    "  :param video_fps: 录制视频回放的每秒帧数\n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "\n",
    "  # 步骤 1: 创建仓库\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "\n",
    "    # 步骤 2: 保存模型\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # 步骤 3: 将超参数保存到 JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "\n",
    "    # 步骤 4: 评估模型并构建 JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env,\n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"],\n",
    "                                            model)\n",
    "    # 获取日期时间\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"],\n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # 写入 JSON 文件\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # 步骤 5: 创建模型卡\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # 添加指标\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # 合并两个字典\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** 智能体玩 **{env_id}**\n",
    "  这是一个训练好的 **Reinforce** 智能体玩 **{env_id}** 的模型。\n",
    "  要学习如何使用此模型并训练你自己的模型，请查看深度强化学习课程的第 4 单元：https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # 将我们的指标保存到 Readme 元数据中\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # 步骤 6: 录制视频\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "    # 步骤 7: 将所有内容推送到 Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"你的模型已推送到 Hub。你可以在这里查看你的模型：{repo_url}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:33:20.854706Z",
     "start_time": "2025-08-12T18:33:20.843722Z"
    }
   },
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w17w8CxzoURM"
   },
   "source": [
    "通过使用 `push_to_hub`，你**评估、录制回放、生成智能体的模型卡并将其推送到 Hub**。\n",
    "\n",
    "这样：\n",
    "- 你可以**展示我们的工作** 🔥\n",
    "- 你可以**可视化你的智能体玩游戏** 👀\n",
    "- 你可以**与社区共享一个其他人可以使用的智能体** 💾\n",
    "- 你可以**访问一个排行榜 🏆 来看看你的智能体与同学相比表现如何** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "为了能够与社区共享你的模型，还需要遵循三个步骤：\n",
    "\n",
    "1️⃣（如果尚未完成）在 HF 上创建一个帐户 ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ 登录，然后，你需要从 Hugging Face 网站存储你的身份验证令牌。\n",
    "- 创建一个新令牌 (https://huggingface.co/settings/tokens) **并赋予写入角色**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QB5nIcxR8paT",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:33:26.303682Z",
     "start_time": "2025-08-12T18:33:26.292696Z"
    }
   },
   "source": [
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bd5376278ff4230b93f0547d9336a3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "如果你不想使用 Jupyter Notebook，你需要改用这个命令：`huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-D-zhbRoeOm"
   },
   "source": [
    "3️⃣ 我们现在准备好使用 `push_to_hub()` 函数将我们训练好的智能体推送到 🤗 Hub 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNwkTS65Uq3Q"
   },
   "outputs": [],
   "source": [
    "# TODO: 定义你的 repo_id {你的用户名/Reinforce-CartPole-v1}\n",
    "# 例如：repo_id = \"ThomasSimonini/Reinforce-CartPole-v1\"\n",
    "repo_id = \"\" \n",
    "push_to_hub(repo_id,\n",
    "                cartpole_policy, # 我们想要保存的模型\n",
    "                cartpole_hyperparameters, # 超参数\n",
    "                eval_env, # 评估环境\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrnuKH1gYZSz"
   },
   "source": [
    "现在我们已经测试了我们实现的鲁棒性，让我们尝试一个更复杂的环境：PixelCopter 🚁\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第二个智能体：PixelCopter 🚁\n",
    "\n",
    "### 研究 PixelCopter 环境 👀\n",
    "- [环境文档](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
   ],
   "metadata": {
    "id": "JNLVmKKVKA6j"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBSc8mlfyin3",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:28:17.856491Z",
     "start_time": "2025-08-12T18:28:17.847420Z"
    }
   },
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"_____观察空间_____ \\n\")\n",
    "print(\"状态空间大小为: \", s_size)\n",
    "print(\"观察样本\", env.observation_space.sample()) # 获取一个随机观察"
   ],
   "metadata": {
    "id": "L5u_zAHsKBy7",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:34:54.758823Z",
     "start_time": "2025-08-12T12:34:54.753542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____观察空间_____ \n",
      "\n",
      "状态空间大小为:  7\n",
      "观察样本 [-0.04964773 -1.9876798  -0.75425804  1.433291   -0.6352314  -0.887026\n",
      "  0.86039406]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\n _____动作空间_____ \\n\")\n",
    "print(\"动作空间大小为: \", a_size)\n",
    "print(\"动作空间样本\", env.action_space.sample()) # 获取一个随机动作"
   ],
   "metadata": {
    "id": "D7yJM9YXKNbq",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:34:56.341535Z",
     "start_time": "2025-08-12T12:34:56.332215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____动作空间_____ \n",
      "\n",
      "动作空间大小为:  2\n",
      "动作空间样本 1\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNWvlyvzalXr"
   },
   "source": [
    "观察空间 (7) 👀:\n",
    "- 玩家 y 坐标\n",
    "- 玩家速度\n",
    "- 玩家到地面的距离\n",
    "- 玩家到天花板的距离\n",
    "- 下一个障碍物与玩家的 x 距离\n",
    "- 下一个障碍物顶部 y 坐标\n",
    "- 下一个障碍物底部 y 坐标\n",
    "\n",
    "动作空间(2) 🎮:\n",
    "- 向上 (按下加速器)\n",
    "- 什么都不做 (不按加速器)\n",
    "\n",
    "奖励函数 💰:\n",
    "- 每通过一个垂直障碍物，它会获得 +1 的正奖励。每次达到终止状态，它会收到 -1 的负奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义新的策略 🧠\n",
    "- 由于环境更复杂，我们需要一个更深层的神经网络"
   ],
   "metadata": {
    "id": "aV1466QP8crz"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I1eBkCiX2X_S",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:34:59.179712Z",
     "start_time": "2025-08-12T12:34:59.172597Z"
    }
   },
   "source": [
    "# 在这里定义一个新的策略类\n",
    "# 与 CartPole 不同，PixelCopter 更复杂\n",
    "# 我们将使用一个具有三层的更深层网络\n",
    "class PixelCopterPolicy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(PixelCopterPolicy, self).__init__()\n",
    "        # 在这里定义三层\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在这里定义前向传播过程\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1QiGCSbBkM"
   },
   "source": [
    "### 定义超参数 ⚙️\n",
    "- 因为这个环境更复杂。\n",
    "- 我们需要更多的神经元，特别是对于隐藏层。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y0uujOR_ypB6",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:35:01.421900Z",
     "start_time": "2025-08-12T12:35:01.417386Z"
    }
   },
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练它\n",
    "- 我们现在准备好训练我们的智能体了 🔥。"
   ],
   "metadata": {
    "id": "wyvXTJWm9GJG"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7mM2P_ckysFE",
    "ExecuteTime": {
     "end_time": "2025-08-12T12:36:34.226633Z",
     "start_time": "2025-08-12T12:36:30.422160Z"
    }
   },
   "source": [
    "# 创建策略并将其放置到设备上\n",
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = PixelCopterPolicy(pixelcopter_hyperparameters[\"state_space\"],pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v1HEqP-fy-Rf",
    "ExecuteTime": {
     "end_time": "2025-08-12T17:56:28.433628Z",
     "start_time": "2025-08-12T12:37:34.491350Z"
    }
   },
   "source": [
    "scores = reinforce(\n",
    "    pixelcopter_policy,\n",
    "    pixelcopter_optimizer,\n",
    "    pixelcopter_hyperparameters[\"n_training_episodes\"],\n",
    "    pixelcopter_hyperparameters[\"max_t\"],\n",
    "    pixelcopter_hyperparameters[\"gamma\"],\n",
    "    1000,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合 1000\t平均分数: 4.45\n",
      "回合 2000\t平均分数: 6.49\n",
      "回合 3000\t平均分数: 9.65\n",
      "回合 4000\t平均分数: 9.38\n",
      "回合 5000\t平均分数: 10.92\n",
      "回合 6000\t平均分数: 13.16\n",
      "回合 7000\t平均分数: 16.60\n",
      "回合 8000\t平均分数: 20.44\n",
      "回合 9000\t平均分数: 21.16\n",
      "回合 10000\t平均分数: 21.29\n",
      "回合 11000\t平均分数: 23.74\n",
      "回合 12000\t平均分数: 25.15\n",
      "回合 13000\t平均分数: 20.83\n",
      "回合 14000\t平均分数: 26.05\n",
      "回合 15000\t平均分数: 21.60\n",
      "回合 16000\t平均分数: 30.59\n",
      "回合 17000\t平均分数: 27.95\n",
      "回合 18000\t平均分数: 27.53\n",
      "回合 19000\t平均分数: 31.01\n",
      "回合 20000\t平均分数: 24.79\n",
      "回合 21000\t平均分数: 33.51\n",
      "回合 22000\t平均分数: 34.43\n",
      "回合 23000\t平均分数: 29.17\n",
      "回合 24000\t平均分数: 33.71\n",
      "回合 25000\t平均分数: 37.35\n",
      "回合 26000\t平均分数: 29.06\n",
      "回合 27000\t平均分数: 21.70\n",
      "回合 28000\t平均分数: 25.08\n",
      "回合 29000\t平均分数: 29.40\n",
      "回合 30000\t平均分数: 36.96\n",
      "回合 31000\t平均分数: 43.21\n",
      "回合 32000\t平均分数: 30.58\n",
      "回合 33000\t平均分数: 43.62\n",
      "回合 34000\t平均分数: 46.33\n",
      "回合 35000\t平均分数: 53.47\n",
      "回合 36000\t平均分数: 37.60\n",
      "回合 37000\t平均分数: 23.46\n",
      "回合 38000\t平均分数: 52.37\n",
      "回合 39000\t平均分数: 54.17\n",
      "回合 40000\t平均分数: 44.95\n",
      "回合 41000\t平均分数: 53.32\n",
      "回合 42000\t平均分数: 46.35\n",
      "回合 43000\t平均分数: 37.40\n",
      "回合 44000\t平均分数: 56.35\n",
      "回合 45000\t平均分数: 58.51\n",
      "回合 46000\t平均分数: 49.61\n",
      "回合 47000\t平均分数: 66.16\n",
      "回合 48000\t平均分数: 67.40\n",
      "回合 49000\t平均分数: 52.13\n",
      "回合 50000\t平均分数: 55.52\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:41:39.600763Z",
     "start_time": "2025-08-12T18:41:39.592102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 保存模型到当前工作目录\n",
    "torch.save(pixelcopter_policy, \"pixelcopter_policy.pt\")\n"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 在 Hub 上发布我们训练好的模型 🔥"
   ],
   "metadata": {
    "id": "8kwFQ-Ip85BE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: 定义你的 repo_id {你的用户名/Reinforce-Pixelcopter-v0}\n",
    "repo_id = \"a1024053774/Reinforce-Pixelcopter-v0\"\n",
    "push_to_hub(\n",
    "    repo_id,\n",
    "    pixelcopter_policy,  # The model we want to save\n",
    "    pixelcopter_hyperparameters,  # Hyperparameters\n",
    "    eval_env,  # Evaluation environment\n",
    "    video_fps=30\n",
    ")"
   ],
   "metadata": {
    "id": "6PtB7LRbTKWK",
    "ExecuteTime": {
     "end_time": "2025-08-12T18:38:52.054513Z",
     "start_time": "2025-08-12T18:38:51.554832Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 7x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[61]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# TODO: 定义你的 repo_id {你的用户名/Reinforce-Pixelcopter-v0}\u001B[39;00m\n\u001B[32m      2\u001B[39m repo_id = \u001B[33m\"\u001B[39m\u001B[33ma1024053774/Reinforce-Pixelcopter-v0\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m push_to_hub(\n\u001B[32m      4\u001B[39m     repo_id,\n\u001B[32m      5\u001B[39m     pixelcopter_policy,  \u001B[38;5;66;03m# The model we want to save\u001B[39;00m\n\u001B[32m      6\u001B[39m     pixelcopter_hyperparameters,  \u001B[38;5;66;03m# Hyperparameters\u001B[39;00m\n\u001B[32m      7\u001B[39m     eval_env,  \u001B[38;5;66;03m# Evaluation environment\u001B[39;00m\n\u001B[32m      8\u001B[39m     video_fps=\u001B[32m30\u001B[39m\n\u001B[32m      9\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[55]\u001B[39m\u001B[32m, line 42\u001B[39m, in \u001B[36mpush_to_hub\u001B[39m\u001B[34m(repo_id, model, hyperparameters, eval_env, video_fps)\u001B[39m\n\u001B[32m     39\u001B[39m   json.dump(hyperparameters, outfile)\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# 步骤 4: 评估模型并构建 JSON\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m mean_reward, std_reward = evaluate_agent(eval_env,\n\u001B[32m     43\u001B[39m                                         hyperparameters[\u001B[33m\"\u001B[39m\u001B[33mmax_t\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     44\u001B[39m                                         hyperparameters[\u001B[33m\"\u001B[39m\u001B[33mn_evaluation_episodes\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     45\u001B[39m                                         model)\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m# 获取日期时间\u001B[39;00m\n\u001B[32m     47\u001B[39m eval_datetime = datetime.datetime.now()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[59]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mevaluate_agent\u001B[39m\u001B[34m(env, max_steps, n_eval_episodes, policy)\u001B[39m\n\u001B[32m     13\u001B[39m total_rewards_ep = \u001B[32m0\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_steps):\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m   action, _ = policy.act(state)\n\u001B[32m     17\u001B[39m   new_state, reward, done, info = env.step(action)\n\u001B[32m     18\u001B[39m   total_rewards_ep += reward\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36mPixelCopterPolicy.act\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mact\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[32m     20\u001B[39m     state = torch.from_numpy(state).float().unsqueeze(\u001B[32m0\u001B[39m).to(device)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     probs = \u001B[38;5;28mself\u001B[39m.forward(state).cpu()\n\u001B[32m     22\u001B[39m     m = Categorical(probs)\n\u001B[32m     23\u001B[39m     action = m.sample()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mPixelCopterPolicy.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m     13\u001B[39m     \u001B[38;5;66;03m# 在这里定义前向传播过程\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     x = F.relu(\u001B[38;5;28mself\u001B[39m.fc1(x))\n\u001B[32m     15\u001B[39m     x = F.relu(\u001B[38;5;28mself\u001B[39m.fc2(x))\n\u001B[32m     16\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.fc3(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.linear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m.weight, \u001B[38;5;28mself\u001B[39m.bias)\n",
      "\u001B[31mRuntimeError\u001B[39m: mat1 and mat2 shapes cannot be multiplied (1x4 and 7x64)"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VDcJ29FcOyb"
   },
   "source": [
    "## 一些额外的挑战 🏆\n",
    "最好的学习方式是**亲自动手尝试**！正如你所看到的，当前的智能体表现并不出色。作为第一个建议，你可以训练更多的步数。但也可以尝试寻找更好的参数。\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)上，你会找到你的智能体。你能登上榜首吗？\n",
    "\n",
    "这里有一些实现这一目标的方法：\n",
    "* 训练更多步数\n",
    "* 通过查看你同学的做法来尝试不同的超参数 👉 https://huggingface.co/models?other=reinforce\n",
    "* **将你新训练的模型推送到 Hub** 🔥\n",
    "* **为更复杂的环境改进实现**（例如，将网络更改为卷积神经网络以处理\n",
    "作为观察的帧怎么样）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62pP0PHdA-y"
   },
   "source": [
    "________________________________________________________________________\n",
    "\n",
    "**恭喜你完成本单元**！这里有很多信息。\n",
    "也恭喜你完成了本教程。你刚刚使用 PyTorch 从头开始编写了你的第一个深度强化学习智能体，并将其分享到了 Hub 上 🥳。\n",
    "\n",
    "不要犹豫，通过**为更复杂的环境改进实现来迭代本单元**（例如，将网络更改为卷积神经网络以处理作为观察的帧怎么样）？\n",
    "\n",
    "在下一个单元中，**我们将通过在 Unity 环境中训练智能体来学习更多关于 Unity MLAgents 的知识**。这样，你将准备好参加 **AI vs AI 挑战赛，在那里你将训练你的智能体\n",
    "在雪球大战和足球比赛中与其他智能体竞争。**\n",
    "\n",
    "听起来很有趣？下次见！\n",
    "\n",
    "最后，我们很想**听听你对课程的看法以及我们如何改进它**。如果你有任何反馈，请 👉  [填写此表格](https://forms.gle/BzKXWzLAGZESGNaE9)\n",
    "\n",
    "我们在第 5 单元见！🔥\n",
    "\n",
    "### 继续学习，保持出色 🤗\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
