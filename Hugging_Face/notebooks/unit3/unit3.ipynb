{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xBVPzoXxOg"
   },
   "source": [
    "# 第 3 单元：使用 RL Baselines3 Zoo 体验 Atari 游戏中的深度 Q-Learning 👾\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
    "\n",
    "在本笔记本中，**你将训练一个玩太空侵略者（Space Invaders）的深度 Q-Learning 智能体**，我们会使用 [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)，这是一个基于 [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) 的训练框架，提供了用于训练、评估智能体、调整超参数、绘制结果和录制视频的脚本。\n",
    "\n",
    "我们使用的是 [RL-Baselines-3 Zoo 集成，一个原生版本的深度 Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)，没有诸如 Double-DQN、Dueling-DQN 和 Prioritized Experience Replay 等扩展。\n",
    "\n",
    "⬇️ 以下是你**将要实现**的示例 ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S713biXntc"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 🎮 环境:\n",
    "\n",
    "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
    "\n",
    "你可以在这里查看太空侵略者不同版本之间的区别 👉 https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
    "\n",
    "### 📚 强化学习库:\n",
    "\n",
    "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
   ],
   "metadata": {
    "id": "ykJiGevCMVc5"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciHGjrFYz9m"
   },
   "source": [
    "## 本笔记本的目标 🏆\n",
    "在完成本笔记本后，你将能够：\n",
    "- 更深入地理解 **RL Baselines3 Zoo 的工作原理**。\n",
    "- 能够**将你训练好的智能体和代码推送到 Hub**，并附上精彩的视频回放和评估分数 🔥。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 本笔记本来自深度强化学习课程\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ],
   "metadata": {
    "id": "TsnP0rjxMn1e"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw6fJHIAZd-J"
   },
   "source": [
    "在这门免费课程中，你将：\n",
    "\n",
    "- 📖 从**理论和实践**两方面学习深度强化学习。\n",
    "- 🧑‍💻 学会**使用著名的深度强化学习库**，如 Stable Baselines3、RL Baselines3 Zoo、CleanRL 和 Sample Factory 2.0。\n",
    "- 🤖 在**独特的环境中训练智能体**\n",
    "\n",
    "更多内容请查看 📚 课程大纲 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "别忘了**<a href=\"http://eepurl.com/ic5ZUD\">注册课程</a>**（我们收集你的电子邮件是为了**在每个单元发布时向你发送链接，并为你提供有关挑战和更新的信息**）。\n",
    "\n",
    "\n",
    "保持联系的最佳方式是加入我们的 Discord 服务器，与社区和我们交流 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vgANIBBZg1p"
   },
   "source": [
    "## 前提条件 🏗️\n",
    "在深入学习本笔记本之前，你需要：\n",
    "\n",
    "🔲 📚 **[通过阅读第 3 单元来学习深度 Q-Learning](https://huggingface.co/deep-rl-course/unit3/introduction)** 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们一直在努力改进我们的教程，所以**如果你在本笔记本中发现任何问题**，请[在 Github 仓库中提出一个 issue](https://github.com/huggingface/deep-rl-class/issues)。"
   ],
   "metadata": {
    "id": "7kszpGFaRVhq"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR0jZtYreSI5"
   },
   "source": [
    "# 让我们来训练一个玩 Atari 太空侵略者 👾 的深度 Q-Learning 智能体，并将其上传到 Hub。\n",
    "\n",
    "我们强烈建议学生**使用 Google Colab 进行动手练习，而不是在个人电脑上运行**。\n",
    "\n",
    "通过使用 Google Colab，**你可以专注于学习和实验，而无需担心设置环境的技术细节**。\n",
    "\n",
    "要为认证过程验证此动手练习，你需要将你训练好的模型推送到 Hub，并**获得 >= 200 的结果**。\n",
    "\n",
    "要找到你的结果，请前往排行榜并找到你的模型，**结果 = mean_reward - std of reward**\n",
    "\n",
    "有关认证过程的更多信息，请查看此部分 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一个建议 💡\n",
    "最好在你的 Google Drive 中运行此 colab 的副本，这样**如果它超时了**，你仍然可以在你的 Google Drive 上保存笔记本，而不需要从头开始填写所有内容。\n",
    "\n",
    "要做到这一点，你可以按 `Ctrl + S` 或 `文件 > 在 Google Drive 中保存一份副本`。\n",
    "\n",
    "另外，我们将**用 100 万个时间步来训练它，大约需要 90 分钟**。通过输入 `!nvidia-smi` 将告诉你正在使用哪种 GPU。\n",
    "\n",
    "如果你想训练更多，比如 1000 万个时间步，这将需要大约 9 个小时，可能会导致 Colab 超时。在这种情况下，我建议你在本地计算机（或其他地方）上运行。只需点击：`文件>下载`。"
   ],
   "metadata": {
    "id": "Nc8BnyVEc3Ys"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 设置 GPU 💪\n",
    "- 为了**加速智能体的训练，我们将使用 GPU**。要做到这一点，请转到 `Runtime > Change Runtime type`\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
   ],
   "metadata": {
    "id": "PU4FVzaoM6fC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- `Hardware Accelerator > GPU`\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
   ],
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 安装 RL-Baselines3 Zoo 及其依赖项 📚\n",
    "\n",
    "如果你看到 `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **这是正常的，并非严重错误**，只是版本冲突。但是我们需要的包已经安装好了。"
   ],
   "metadata": {
    "id": "wS_cVefO-aYg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
   ],
   "metadata": {
    "id": "S1A_E4z3awa_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!apt-get install swig cmake ffmpeg"
   ],
   "metadata": {
    "id": "8_MllY6Om1eI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S9mJiKg6SqC"
   },
   "source": [
    "为了能够在 Gymnasium 中使用 Atari 游戏，我们需要安装 atari 包。并使用 accept-rom-license 来下载 rom 文件（游戏文件）。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]"
   ],
   "metadata": {
    "id": "NsRP-lX1_2fC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建一个虚拟显示器 🔽\n",
    "\n",
    "在笔记本的运行过程中，我们需要生成一个回放视频。为此，在 colab 中，**我们需要一个虚拟屏幕来渲染环境**（从而录制帧）。\n",
    "\n",
    "因此，下面的单元格将安装相关库并创建和运行一个虚拟屏幕 🖥"
   ],
   "metadata": {
    "id": "bTpYcVZVMzUI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 虚拟显示器\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ],
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iPgzluo9z-u"
   },
   "source": [
    "## 训练我们的深度 Q-Learning 智能体来玩太空侵略者 👾\n",
    "\n",
    "要使用 RL-Baselines3-Zoo 训练一个智能体，我们只需要做两件事：\n",
    "\n",
    "1. 创建一个名为 `dqn.yml` 的超参数配置文件，其中包含我们的训练超参数。\n",
    "\n",
    "这是一个模板示例：\n",
    "\n",
    "```\n",
    "SpaceInvadersNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e6\n",
    "  buffer_size: 100000\n",
    "  learning_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  learning_starts: 100000\n",
    "  target_update_interval: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  exploration_fraction: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  # If True, you need to deactivate handle_timeout_termination\n",
    "  # in the replay_buffer_kwargs\n",
    "  optimize_memory_usage: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VjblFSVDQOj"
   },
   "source": [
    "这里我们可以看到：\n",
    "- 我们使用了 `Atari Wrapper`，它对输入进行预处理（帧缩减、灰度化、堆叠4帧）\n",
    "- 我们使用了 `CnnPolicy`，因为我们用卷积层来处理帧\n",
    "- 我们训练了 1000 万个 `n_timesteps`\n",
    "- 内存（经验回放）大小为 100000，也就是你保存的用于再次训练智能体的经验步数。\n",
    "\n",
    "💡 我的建议是**将训练时间步数减少到 100 万**，在 P100 上大约需要 90 分钟。`!nvidia-smi` 会告诉你正在使用哪种 GPU。如果设置 1000 万步，大约需要 9 个小时，这很可能导致 Colab 超时。我建议你在本地计算机（或其他地方）上运行。只需点击：`文件>下载`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTkbWrkECOJ"
   },
   "source": [
    "在超参数优化方面，我的建议是关注这 3 个超参数：\n",
    "- `learning_rate`（学习率）\n",
    "- `buffer_size`（经验记忆大小）\n",
    "- `batch_size`（批次大小）\n",
    "\n",
    "作为一个好的实践，你需要**查阅文档来理解每个超参数的作用**：https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn8bRTHvERRL"
   },
   "source": [
    "2. 我们开始训练，并将模型保存在 `logs` 文件夹中 📁\n",
    "\n",
    "- 在 `--algo` 后面定义算法，在 `-f` 后面指定模型保存位置，在 `-c` 后面指定超参数配置文件。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xr1TVW4xfbz3",
    "ExecuteTime": {
     "end_time": "2025-08-10T14:39:55.636669Z",
     "start_time": "2025-08-10T14:39:35.461868Z"
    }
   },
   "source": [
    "!python -m rl_zoo3.train --algo ________ --env SpaceInvadersNoFrameskip-v4  -f _________  -c _________"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:39:40.186523: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-10 22:39:41.676486: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "usage: train.py [-h]\n",
      "                [--algo {a2c,ddpg,dqn,ppo,sac,td3,ars,crossq,qrdqn,tqc,trpo,ppo_lstm}]\n",
      "                [--env ENV] [-tb TENSORBOARD_LOG] [-i TRAINED_AGENT]\n",
      "                [--truncate-last-trajectory TRUNCATE_LAST_TRAJECTORY]\n",
      "                [-n N_TIMESTEPS] [--num-threads NUM_THREADS]\n",
      "                [--log-interval LOG_INTERVAL] [--eval-freq EVAL_FREQ]\n",
      "                [--optimization-log-path OPTIMIZATION_LOG_PATH]\n",
      "                [--eval-episodes EVAL_EPISODES] [--n-eval-envs N_EVAL_ENVS]\n",
      "                [--save-freq SAVE_FREQ] [--save-replay-buffer] [-f LOG_FOLDER]\n",
      "                [--seed SEED] [--vec-env {dummy,subproc}] [--device DEVICE]\n",
      "                [--n-trials N_TRIALS] [--max-total-trials MAX_TOTAL_TRIALS]\n",
      "                [-optimize] [--no-optim-plots] [--n-jobs N_JOBS]\n",
      "                [--sampler {random,tpe,auto}] [--pruner {halving,median,none}]\n",
      "                [--n-startup-trials N_STARTUP_TRIALS]\n",
      "                [--n-evaluations N_EVALUATIONS] [--storage STORAGE]\n",
      "                [--study-name STUDY_NAME] [--trial-id TRIAL_ID]\n",
      "                [--verbose VERBOSE]\n",
      "                [--gym-packages GYM_PACKAGES [GYM_PACKAGES ...]]\n",
      "                [--env-kwargs ENV_KWARGS [ENV_KWARGS ...]]\n",
      "                [--eval-env-kwargs EVAL_ENV_KWARGS [EVAL_ENV_KWARGS ...]]\n",
      "                [-params HYPERPARAMS [HYPERPARAMS ...]] [-conf CONF_FILE]\n",
      "                [-uuid] [--track] [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "                [--wandb-entity WANDB_ENTITY] [-P]\n",
      "                [-tags WANDB_TAGS [WANDB_TAGS ...]]\n",
      "train.py: error: argument --algo: invalid choice: '________' (choose from 'a2c', 'ddpg', 'dqn', 'ppo', 'sac', 'td3', 'ars', 'crossq', 'qrdqn', 'tqc', 'trpo', 'ppo_lstm')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeChoX-3SZfP"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T14:45:31.245201Z",
     "start_time": "2025-08-10T14:44:55.173956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 安装支持 Atari 环境的 gymnasium\n",
    "!pip install \"gymnasium[atari]\"\n",
    "\n",
    "# 安装 Atari 游戏 ROMs 的 Python 接口\n",
    "!pip install \"ale-py==0.8.1\"\n",
    "\n",
    "!pip install gymnasium[accept-rom-license]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[atari]) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[atari]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gymnasium[atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: ale_py>=0.9 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[atari]) (0.11.1)\n",
      "Collecting ale-py==0.8.1\n",
      "  Downloading ale_py-0.8.1-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from ale-py==0.8.1) (2.0.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from ale-py==0.8.1) (6.5.2)\n",
      "Downloading ale_py-0.8.1-cp311-cp311-win_amd64.whl (952 kB)\n",
      "   ---------------------------------------- 0.0/952.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 952.4/952.4 kB 11.0 MB/s eta 0:00:00\n",
      "Installing collected packages: ale-py\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.11.1\n",
      "    Uninstalling ale-py-0.11.1:\n",
      "      Successfully uninstalled ale-py-0.11.1\n",
      "Successfully installed ale-py-0.8.1\n",
      "Requirement already satisfied: gymnasium[accept-rom-license] in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[accept-rom-license]) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[accept-rom-license]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium[accept-rom-license]) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PuocgdokSab9",
    "ExecuteTime": {
     "end_time": "2025-08-10T14:46:37.759450Z",
     "start_time": "2025-08-10T14:46:15.916976Z"
    }
   },
   "source": [
    "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:46:20.604859: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-10 22:46:22.014120: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"D:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\rl_zoo3\\train.py\", line 286, in <module>\n",
      "    train()\n",
      "  File \"D:\\DevelopmentSoftware\\Anaconda\\envs\\py311\\Lib\\site-packages\\rl_zoo3\\train.py\", line 181, in train\n",
      "    raise ValueError(f\"{env_id} not found in gym registry, you maybe meant {closest_match}?\")\n",
      "ValueError: SpaceInvadersNoFrameskip-v4 not found in gym registry, you maybe meant 'no close match found...'?\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dLomIiMKQaf"
   },
   "source": [
    "## 让我们来评估我们的智能体 👀\n",
    "- RL-Baselines3-Zoo 提供了 `enjoy.py`，这是一个用于评估我们智能体的 python 脚本。在大多数强化学习库中，我们都将评估脚本称为 `enjoy.py`。\n",
    "- 让我们用 5000 个时间步来评估它 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co5um_KeKbBJ"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q24K1tyWSj7t"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_uSmwGRSk0z"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liBeTltiHJtr"
   },
   "source": [
    "## 将我们训练好的模型发布到 Hub 🚀\n",
    "既然我们看到训练后取得了不错的结果，我们就可以用一行代码将我们训练好的模型发布到 Hub 🤗。\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbHS1q3HYVV"
   },
   "source": [
    "通过使用 `rl_zoo3.push_to_hub`，**你可以评估、录制回放、生成你的智能体的模型卡，并将其推送到 Hub**。\n",
    "\n",
    "这样一来：\n",
    "- 你可以**展示你的工作** 🔥\n",
    "- 你可以**可视化你的智能体玩游戏的过程** 👀\n",
    "- 你可以**与社区分享一个其他人可以使用的智能体** 💾\n",
    "- 你可以**访问一个排行榜 🏆 来看看你的智能体与同学们的表现如何** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMSeZRBiHk6X"
   },
   "source": [
    "为了能与社区分享你的模型，还需要遵循以下三个步骤：\n",
    "\n",
    "1️⃣ （如果还没做的话）在 HF 上创建一个账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ 登录，然后你需要从 Hugging Face 网站保存你的身份验证令牌。\n",
    "- 创建一个**具有写入权限**的新令牌 (https://huggingface.co/settings/tokens)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O6FI0F8HnzE"
   },
   "source": [
    "- 复制令牌\n",
    "- 运行下面的单元格并粘贴令牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu9yePwHrZX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # 登录我们的 Hugging Face 账户，以便能够上传模型到 Hub。\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RVEdunPHs8B"
   },
   "source": [
    "如果你不想使用 Google Colab 或 Jupyter Notebook，你需要使用这个命令： `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSLwdmvhHvjw"
   },
   "source": [
    "3️⃣ 现在我们准备好将我们训练好的智能体推送到 🤗 Hub 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW436XnhHw1H"
   },
   "source": [
    "让我们运行 push_to_hub.py 文件将我们训练好的智能体上传到 Hub。\n",
    "\n",
    "`--repo-name `: 仓库的名称\n",
    "\n",
    "`-orga`: 你的 Hugging Face 用户名\n",
    "\n",
    "`-f`: 训练好的模型文件夹所在的位置（在我们的例子中是 `logs`）\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygk2sEktTDEw"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgpa0rhS9wR"
   },
   "source": [
    "#### 解决方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HQNlAXuEhci"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D4F5zsTTJ-L"
   },
   "source": [
    "###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff89kd2HL1_s"
   },
   "source": [
    "恭喜 🥳 你刚刚使用 RL-Baselines-3 Zoo 训练并上传了你的第一个深度 Q-Learning 智能体。上面的脚本应该已经显示了一个模型仓库的链接，例如 https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4。当你访问这个链接时，你可以：\n",
    "\n",
    "- 在右侧看到**你的智能体的视频预览**。\n",
    "- 点击“Files and versions”查看仓库中的所有文件。\n",
    "- 点击“Use in stable-baselines3”获取一个代码片段，展示如何加载模型。\n",
    "- 一个模型卡（`README.md` 文件），其中给出了模型的描述和你使用的超参数。\n",
    "\n",
    "在底层，Hub 使用基于 git 的仓库（如果你不知道 git 是什么也没关系），这意味着你可以随着实验和改进你的智能体，用新版本更新模型。\n",
    "\n",
    "**使用[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 🏆 与你的同学比较你的智能体的结果**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyRKcCYY-dIo"
   },
   "source": [
    "## 加载一个强大的训练模型 🔥\n",
    "- Stable-Baselines3 团队在 Hub 上传了**超过 150 个训练好的深度强化学习智能体**。\n",
    "\n",
    "你可以在这里找到它们：👉 https://huggingface.co/sb3\n",
    "\n",
    "一些例子：\n",
    "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
    "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
    "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
    "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
    "\n",
    "让我们加载一个玩 Beam Rider 的智能体：https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-9QVFIROI5Y"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZQNY_r6NJtC"
   },
   "source": [
    "1. 我们使用 `rl_zoo3.load_from_hub` 下载模型，并将其放在一个我们可以称为 `rl_trained` 的新文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdBNZHy0NGTR"
   },
   "outputs": [],
   "source": [
    "# 下载模型并将其保存到 logs/ 文件夹中\n",
    "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFt6hmWsNdBo"
   },
   "source": [
    "2. 让我们用 5000 个时间步来评估它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxs0rNuN0uS"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMDuDfPON57"
   },
   "source": [
    "为什么不尝试训练你自己的**玩 BeamRiderNoFrameskip-v4 的深度 Q-Learning 智能体呢？🏆.**\n",
    "\n",
    "如果你想尝试，请查看 https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **在模型卡中，有训练好的智能体的超参数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_ZtUgpOuY6"
   },
   "source": [
    "但是寻找超参数可能是一项艰巨的任务。幸运的是，我们将在下一个单元中学习如何**使用 Optuna 来优化超参数 🔥.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pqaco8W-huW"
   },
   "source": [
    "## 一些额外的挑战 🏆\n",
    "学习的最好方式**就是亲自动手尝试**！\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)上，你会找到你的智能体。你能登上榜首吗？\n",
    "\n",
    "这里有一些你可以尝试训练智能体的环境：\n",
    "- BeamRiderNoFrameskip-v4\n",
    "- BreakoutNoFrameskip-v4\n",
    "- EnduroNoFrameskip-v4\n",
    "- PongNoFrameskip-v4\n",
    "\n",
    "此外，**如果你想学习如何自己实现深度 Q-Learning**，你绝对应该看看 CleanRL 的实现：https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS-XKo4-kmu"
   },
   "source": [
    "________________________________________________________________________\n",
    "恭喜完成本章！\n",
    "\n",
    "如果你仍然对所有这些元素感到困惑……这完全正常！**我和所有学习强化学习的人都有过同样的经历。**\n",
    "\n",
    "花点时间真正**掌握这些材料，然后再继续，并尝试额外的挑战**。掌握这些元素并打下坚实的基础非常重要。\n",
    "\n",
    "在下一个单元中，**我们将学习 [Optuna](https://optuna.org/)**。深度强化学习中最关键的任务之一是找到一组好的训练超参数。而 Optuna 是一个能帮助你自动化搜索过程的库。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRx7tO7-mvC"
   },
   "source": [
    "\n",
    "\n",
    "### 这是一门与你共同打造的课程 👷🏿‍♀️\n",
    "\n",
    "最后，我们希望根据你的反馈来迭代地改进和更新课程。如果你有任何意见，请填写此表格 👉 https://forms.gle/3HgA7bEHwAmmLfwh9\n",
    "\n",
    "我们一直在努力改进我们的教程，所以**如果你在本笔记本中发现任何问题**，请[在 Github 仓库中提出一个 issue](https://github.com/huggingface/deep-rl-class/issues)。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们奖励单元 2 见！ 🔥"
   ],
   "metadata": {
    "id": "Kc3udPT-RcXc"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3Xerx0fIMV"
   },
   "source": [
    "### 保持学习，保持出色 🤗"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
