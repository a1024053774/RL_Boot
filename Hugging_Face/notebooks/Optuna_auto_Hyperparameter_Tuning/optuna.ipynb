{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# 使用 Optuna 进行超参数调优\n",
    "\n",
    "本教程的 Github 仓库: https://github.com/araffin/tools-for-robotic-rl-icra2022\n",
    "\n",
    "相关链接:\n",
    "- Optuna: https://github.com/optuna/optuna\n",
    "- Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "- 官方文档: https://stable-baselines3.readthedocs.io/en/master/\n",
    "- SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "- RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) 是一个使用 Stable-Baselines3 预训练的强化学习智能体的集合。\n",
    "\n",
    "它还提供了用于训练、评估智能体、调优超参数和录制视频的基础脚本。\n",
    "\n",
    "## 引言\n",
    "\n",
    "在本 Notebook 中，你将学习到调优超参数的重要性。你将首先尝试手动优化参数，然后我们将看到如何使用 Optuna 自动化搜索过程。\n",
    "\n",
    "## 为本地运行设置环境 (建议)\n",
    "\n",
    "为了避免包版本冲突，强烈建议你创建一个独立的虚拟环境。你可以使用 `conda` 或 `venv`。\n",
    "\n",
    "**使用 Conda:**\n",
    "```bash\n",
    "conda create -n optuna_lab python=3.9\n",
    "conda activate optuna_lab\n",
    "```\n",
    "\n",
    "**使用 venv:**\n",
    "```bash\n",
    "python -m venv optuna_lab\n",
    "source optuna_lab/bin/activate  # 在 Windows 上使用 `optuna_lab\\Scripts\\activate`\n",
    "```\n",
    "\n",
    "创建并激活环境后，你可以运行下面的安装命令来安装所有必需的依赖项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dependencies-install"
   },
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hYdv2ygjLaFL",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 安装 Stable-Baselines3 核心库\n",
    "!pip install stable-baselines3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oexj67yWN5_k",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 可选：安装 SB3 contrib 以使用额外的算法\n",
    "!pip install sb3-contrib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NNah91r9x9EL",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 安装 Optuna，我们将在最后一部分用它来进行超参数调优\n",
    "!pip install optuna"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BIedd7Pz9sOs",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "import gym\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae32CtgzTG3R"
   },
   "source": [
    "首先你需要导入强化学习模型，请查阅官方文档以了解在什么问题上可以使用哪些模型。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R7tKaBFrTR0a",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EcsXmYRMON9W",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 来自 Contrib 仓库的算法\n",
    "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "from sb3_contrib import QRDQN, TQC"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kLwjcfvuqtGE",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-khNkrgcI6Z1"
   },
   "source": [
    "# 第一部分：调优超参数的重要性\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PytOtL9GdmrE"
   },
   "source": [
    "与监督学习相比，深度强化学习对超参数（如学习率、神经元数量、层数、优化器等）的选择要敏感得多。\n",
    "\n",
    "糟糕的超参数选择可能导致模型收敛效果差或不稳定。此外，由于随机种子（用于初始化网络权重和环境）的不同，模型性能的差异性也加剧了这一挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk8HSIC3qUjc"
   },
   "source": [
    "除了超参数，选择合适的算法也是一个重要的决定。我们将在简单的“倒立摆”任务（Pendulum）上进行演示。\n",
    "\n",
    "参考 [gym 文档](https://gym.openai.com/envs/Pendulum-v0/)：“倒立摆上摆问题是控制文献中的一个经典问题。在这个版本的问题中，倒立摆从一个随机位置开始，目标是将其向上摆动并保持直立。”\n",
    "\n",
    "\n",
    "我们先来试试 PPO 算法，并给它一个 4000 步（约 20 个回合）的少量训练预算："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ToIvihGq2N0",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "env_id = \"Pendulum-v1\"\n",
    "# 这个环境只用于评估\n",
    "eval_envs = make_vec_env(env_id, n_envs=10)\n",
    "# 4000 个训练时间步\n",
    "budget_pendulum = 4000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWT2r6QE4yew"
   },
   "source": [
    "### PPO 算法"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KCHk_-_4ndux",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TP9C9AqLndxz",
    "outputId": "dd8e423c-dd4d-43cf-eac5-639e6748f02c",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"PPO 平均回合奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHmJaJLl5ds4"
   },
   "source": [
    "### A2C 算法 (练习)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BLL_pws25jh0",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 定义并训练一个 A2C 模型\n",
    "a2c_model = A2C(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ic83jZwB5nVk",
    "vscode": {
     "languageId": "python"
    },
    "outputs": []
   },
   "source": [
    "# 评估训练好的 A2C 模型\n",
    "mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"A2C 平均回合奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_z1zFx2rVpG"
   },
   "source": [
    "两种算法的得分都远未解决这个问题（平均奖励约为 -200 才算解决）。\n",
    "现在，我们来试试离线策略（off-policy）算法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wYaVZJU5VL5"
   },
   "source": [
    "### 延长 PPO 的训练时间？\n",
    "\n",
    "也许训练更长时间会有帮助？\n",
    "\n",
    "你可以尝试将训练预算增加10倍，但对于 A2C/PPO 来说，延长训练时间帮助不大，真正需要的是找到更好的超参数。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hHsHpnQY6TWA",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 延长训练时间\n",
    "new_budget = 10 * budget_pendulum\n",
    "\n",
    "ppo_model_long = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(new_budget)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7OD9y1o36Xta",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_model_long, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"PPO (长时训练) 平均回合奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEvQ9SJ15Xmh"
   },
   "source": [
    "### PPO - 调优后的超参数\n",
    "\n",
    "实际上，我们可以使用 Optuna 来调优超参数，并找到一个有效的解决方案（来自 [RL Zoo](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml)）:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S-D_vvsb6jOZ",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "tuned_params = {\n",
    "    \"gamma\": 0.9,\n",
    "    \"use_sde\": True,\n",
    "    \"sde_sample_freq\": 4,\n",
    "    \"learning_rate\": 1e-3,\n",
    "}\n",
    "\n",
    "ppo_tuned_model = PPO(\"MlpPolicy\", env_id, seed=1, verbose=1, **tuned_params).learn(50_000, log_interval=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLuxoLxt67xO",
    "outputId": "6bc7479b-689f-4d0f-9f01-379c31afdb4e",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"调优后的 PPO 平均回合奖励: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H33u_apWPp5"
   },
   "source": [
    "注意：如果你在简单的 `MountainCarContinuous` 环境上尝试 SAC 算法，而不使用调优过的超参数，你会遇到一些问题：https://github.com/rail-berkeley/softlearning/issues/76\n",
    "\n",
    "即便是简单的环境，对于 SOTA (State-of-the-art) 算法来说也可能具有挑战性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vdpPJ04nebx"
   },
   "source": [
    "# 第二部分：研究生梯度下降 (手动调参)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8PNN9kcgolk"
   },
   "source": [
    "### 挑战 (10分钟): \"研究生梯度下降\"\n",
    "这个挑战是在有限的 20,000 个训练步数的预算下，为 A2C 算法在 `CartPole-v1` 环境中找到最佳的超参数（以获得最高性能）。\n",
    "\n",
    "`CartPole-v1` 的最高奖励是 500。\n",
    "\n",
    "你找到的超参数应该在不同的随机种子下都能表现良好。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s6aqxsini7H3",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "budget = 20_000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDQ805DBi3KM"
   },
   "source": [
    "#### 基线: 默认超参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pyOCKf4Vt-HK",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "eval_envs_cartpole = make_vec_env(\"CartPole-v1\", n_envs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D1PSNGcsi2dP",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=0).learn(budget)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d3X0G0ng2OE",
    "outputId": "8d550b14-a673-4abd-b9b8-c539d9c79c05",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n",
    "\n",
    "print(f\"平均奖励:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-fi1-oKnUI2"
   },
   "source": [
    "**你的目标是超越这个基线性能，并尽可能接近 500 的最优分数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvq8zizok1X_"
   },
   "source": [
    "开始调参吧！"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UaqCCH4gkRH_",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uDUfeZcyjPKS",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "policy_kwargs = dict(\n",
    "    # 为 actor/critic 定义网络结构\n",
    "    net_arch=[\n",
    "      dict(vf=[64, 64], pi=[64, 64]),\n",
    "    ],\n",
    "    # 激活函数\n",
    "    activation_fn=nn.Tanh,\n",
    ")\n",
    "\n",
    "# 尝试调整这些超参数！\n",
    "hyperparams = dict(\n",
    "    n_steps=7, # 在更新策略前收集数据的步数\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.99, # 折扣因子\n",
    "    max_grad_norm=0.4, # 梯度裁剪的最大值\n",
    "    ent_coef=0.0, # 损失计算中的熵系数\n",
    ")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=0, policy_kwargs=policy_kwargs, **hyperparams).learn(budget)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "python"
    },
    "id": "9L43cxsCu4UA"
   },
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n",
    "\n",
    "print(f\"手动调参后的平均奖励:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL_G9DurUV75"
   },
   "source": [
    "#### 提示 - 推荐的超参数范围\n",
    "\n",
    "这里是一些在自动调参时常用的参数范围，可以给你一些手动调参的灵感。\n",
    "\n",
    "```python\n",
    "# 折扣因子 gamma\n",
    "gamma = trial.suggest_float(\"gamma\", 0.9, 0.99999, log=True)\n",
    "# 梯度裁剪\n",
    "max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "# 更新步数 (从 2**3=8 到 2**10=1024)\n",
    "n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "# 学习率\n",
    "learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "# 熵系数\n",
    "ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "\n",
    "# 网络结构\n",
    "# net_arch tiny: {\"pi\": [64], \"vf\": [64]}\n",
    "# net_arch default: {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "# 激活函数\n",
    "# activation_fn = nn.Tanh / nn.ReLU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwFOp0j-ga-_"
   },
   "source": [
    "# 第三部分：自动超参数调优\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88x7wMyyud5p"
   },
   "source": [
    "在这一部分，我们将创建一个脚本来自动搜索最佳超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auwR-30IvHeY"
   },
   "source": [
    "### 导入 Optuna 相关库"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VM6tUr-yuekR",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQVfmM1dzA1d"
   },
   "source": [
    "### 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yyBTVcAGzCRk",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "N_TRIALS = 100  # 最大试验次数\n",
    "N_JOBS = 1 # 并行运行的任务数\n",
    "N_STARTUP_TRIALS = 5  # 在 N_STARTUP_TRIALS 次试验后停止随机采样\n",
    "N_EVALUATIONS = 2  # 训练过程中的评估次数\n",
    "N_TIMESTEPS = int(2e4)  # 训练预算\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 5 # 评估环境的数量\n",
    "N_EVAL_EPISODES = 10 # 每次评估的回合数\n",
    "TIMEOUT = int(60 * 15)  # 15 分钟超时\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25HgcDYzvJ0b"
   },
   "source": [
    "### 练习 (5分钟): 定义搜索空间"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KXo8AwGAvN8Q",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    A2C 超参数的采样器。\n",
    "\n",
    "    :param trial: Optuna 的 trial 对象\n",
    "    :return: 为给定 trial 采样的超参数。\n",
    "    \"\"\"\n",
    "    # 折扣因子在 0.9 到 0.9999 之间\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    # 8, 16, 32, ... 1024\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "\n",
    "    ### 这里是练习的答案 ###\n",
    "    # - 定义学习率的搜索空间 [1e-5, 1] (对数尺度) -> `suggest_float`\n",
    "    # - 定义网络结构的搜索空间 [\"tiny\", \"small\"] -> `suggest_categorical`\n",
    "    # - 定义激活函数的搜索空间 [\"tanh\", \"relu\"]\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    ### 答案结束 ###\n",
    "\n",
    "    # 在 Optuna 的界面中显示真实值，方便查看\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]}\n",
    "        if net_arch == \"tiny\"\n",
    "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "        },\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iybymNiJxNu7"
   },
   "source": [
    "### 定义目标函数 (Objective Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJY8Z8tuxai7"
   },
   "source": [
    "首先，我们定义一个自定义的回调函数（Callback），用于向 Optuna 报告周期性评估的结果："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U5ijWTPzxSmd",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    用于评估和报告 trial 结果的回调函数。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # 调用父类的方法来评估策略\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # 向 Optuna 报告结果\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # 如果需要，剪枝 trial\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cHNM_cFO3vs"
   },
   "source": [
    "### 练习 (10分钟): 定义目标函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76voi9AXxlCq"
   },
   "source": [
    "然后我们定义目标函数，它负责采样超参数、创建模型，并将结果返回给 Optuna。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E0yEokTDxhrC",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Optuna 使用的目标函数，用于评估一组配置（即一组超参数）。\n",
    "    \n",
    "    给定一个 trial 对象，它会采样超参数，\n",
    "    对其进行评估，并报告结果（训练后的平均回合奖励）。\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    \n",
    "    ### 这里是练习的答案 ###\n",
    "    # 1. 采样超参数并更新默认的关键字参数\n",
    "    params = sample_a2c_params(trial)\n",
    "    kwargs.update(params)\n",
    "\n",
    "    # 创建强化学习模型\n",
    "    model = A2C(**kwargs)\n",
    "\n",
    "    # 2. 使用 `make_vec_env`、`ENV_ID` 和 `N_EVAL_ENVS` 创建用于评估的环境\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "\n",
    "    # 3. 创建上面定义的 `TrialEvalCallback` 回调函数\n",
    "    # 它会每隔 `EVAL_FREQ` 步使用 `N_EVAL_EPISODES` 来周期性地评估和报告性能\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_envs,\n",
    "        trial,\n",
    "        n_eval_episodes=N_EVAL_EPISODES,\n",
    "        eval_freq=EVAL_FREQ,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    ### 答案结束 ###\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # 训练模型\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # 有时，随机的超参数可能会产生 NaN (Not a Number)\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # 释放内存\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # 告诉优化器该 trial 失败了\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMFLu_M0ymzj"
   },
   "source": [
    "### 优化循环"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4UU17YpjymPr",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "import torch as th\n",
    "\n",
    "# 为了更快的训练，将 PyTorch 的线程数设置为 1\n",
    "th.set_num_threads(1)\n",
    "# 选择采样器，可以是 random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# 在使用最大预算的 1/3 之前不进行剪枝\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# 创建 study 并开始超参数优化\n",
    "# `direction` 设置为 'maximize' 因为我们的目标是最大化奖励\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"完成的试验次数: \", len(study.trials))\n",
    "\n",
    "print(\"最佳试验:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  价值 (Value): {trial.value}\")\n",
    "\n",
    "print(\"  参数 (Params): \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  用户自定义属性 (User attrs):\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# 保存研究结果\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "# 可视化结果\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCbep6z1h3D1"
   },
   "source": [
    "一个更完整的示例可以在这里找到： https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yUeYnfJVpB2"
   },
   "source": [
    "# 结论\n",
    "\n",
    "在本 Notebook 中我们学到了：\n",
    "- 好的超参数的重要性\n",
    "- 如何使用 Optuna 进行自动超参数搜索\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3-gqIPXqV7zZ",
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "name": "icra22_optuna_lab_chinese.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
