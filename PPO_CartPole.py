import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import gymnasium as gym
from torch.distributions.categorical import Categorical
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
from dataclasses import dataclass

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")


# å®šä¹‰è¶…å‚æ•°
@dataclass
class Args:
    env_id: str = "CartPole-v1"
    num_envs: int = 4
    num_steps: int = 128
    total_timesteps: int = 1000000
    learning_rate: float = 2.5e-4
    anneal_lr: bool = True
    gamma: float = 0.99
    gae_lambda: float = 0.95
    clip_coef: float = 0.2
    clip_vloss: bool = True
    update_epochs: int = 4
    minibatch_size: int = 32
    norm_adv: bool = True
    ent_coef: float = 0.01
    vf_coef: float = 0.5
    max_grad_norm: float = 0.5
    target_kl: float = None


args = Args()


# ç¯å¢ƒçŠ¶æ€è·Ÿè¸ªå™¨
class EpisodeStatsWrapper(gym.Wrapper):
    """ç®€å•çš„å›åˆç»Ÿè®¡åŒ…è£…å™¨"""

    def __init__(self, env):
        super().__init__(env)
        self.reset_stats()

    def reset_stats(self):
        self.episode_return = 0
        self.episode_length = 0

    def step(self, action):
        obs, reward, done, truncated, info = self.env.step(action)
        self.episode_return += reward
        self.episode_length += 1

        # å›åˆç»“æŸæ—¶æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if done or truncated:
            info = info or {}
            info['episode_stats'] = {
                'return': self.episode_return,
                'length': self.episode_length
            }
            self.reset_stats()

        return obs, reward, done, truncated, info

    def reset(self, **kwargs):
        self.reset_stats()
        return self.env.reset(**kwargs)


def make_env(env_id, seed):
    """åˆ›å»ºç¯å¢ƒå‡½æ•°"""

    def thunk():
        env = gym.make(env_id)
        env = EpisodeStatsWrapper(env)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
        return env

    return thunk


# åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
envs = gym.vector.SyncVectorEnv([make_env(args.env_id, i) for i in range(args.num_envs)])

print(f"ç¯å¢ƒ: {args.env_id}")
print(f"è§‚å¯Ÿç©ºé—´: {envs.single_observation_space}")
print(f"åŠ¨ä½œç©ºé—´: {envs.single_action_space}")


# ç½‘ç»œåˆå§‹åŒ–
def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer


# PPOæ™ºèƒ½ä½“ç½‘ç»œ
class PPOAgent(nn.Module):
    def __init__(self, envs):
        super().__init__()
        obs_dim = np.array(envs.single_observation_space.shape).prod()
        n_actions = envs.single_action_space.n

        # ç‰¹å¾æå–ç½‘ç»œ
        self.network = nn.Sequential(
            layer_init(nn.Linear(obs_dim, 64)),
            nn.Tanh(),
            layer_init(nn.Linear(64, 64)),
            nn.Tanh(),
        )

        # Actorå’ŒCriticå¤´
        self.actor = layer_init(nn.Linear(64, n_actions), std=0.01)
        self.critic = layer_init(nn.Linear(64, 1), std=1)

    def get_value(self, x):
        return self.critic(self.network(x))

    def get_action_and_value(self, x, action=None):
        hidden = self.network(x)
        logits = self.actor(hidden)
        probs = Categorical(logits=logits)

        if action is None:
            action = probs.sample()

        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)


# GAEè®¡ç®—
def compute_advantages(rewards, values, dones, next_value, gamma=0.99, gae_lambda=0.95):
    """è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°"""
    num_steps = rewards.shape[0]
    advantages = torch.zeros_like(rewards)
    lastgaelam = 0

    for t in reversed(range(num_steps)):
        if t == num_steps - 1:
            nextnonterminal = 1.0 - dones[t]
            nextvalues = next_value
        else:
            nextnonterminal = 1.0 - dones[t + 1]
            nextvalues = values[t + 1]

        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]
        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam

    returns = advantages + values
    return advantages, returns


# åˆ›å»ºæ™ºèƒ½ä½“å’Œä¼˜åŒ–å™¨
agent = PPOAgent(envs).to(device)
optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)

# è®¡ç®—è®­ç»ƒå‚æ•°
args.batch_size = args.num_envs * args.num_steps
num_updates = args.total_timesteps // args.batch_size

print(f"æ‰¹é‡å¤§å°: {args.batch_size}")
print(f"æ›´æ–°æ¬¡æ•°: {num_updates}")

# åˆå§‹åŒ–å­˜å‚¨
obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
values = torch.zeros((args.num_steps, args.num_envs)).to(device)


# ç¯å¢ƒçŠ¶æ€è·Ÿè¸ª
class EpisodeTracker:
    def __init__(self):
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_returns = [0] * args.num_envs
        self.current_lengths = [0] * args.num_envs

    def step(self, rewards, dones, infos):
        """æ›´æ–°å›åˆç»Ÿè®¡"""
        for i in range(args.num_envs):
            self.current_returns[i] += rewards[i].item()
            self.current_lengths[i] += 1

            # æ£€æŸ¥å›åˆæ˜¯å¦ç»“æŸ
            if dones[i]:
                # è®°å½•å›åˆæ•°æ®
                self.episode_rewards.append(self.current_returns[i])
                self.episode_lengths.append(self.current_lengths[i])

                # é‡ç½®è®¡æ•°å™¨
                self.current_returns[i] = 0
                self.current_lengths[i] = 0

                return self.current_returns[i], self.current_lengths[i]
        return None, None


# åˆå§‹åŒ–è®­ç»ƒ
episode_tracker = EpisodeTracker()
writer = SummaryWriter("runs/PPO_CartPole_clean")

global_step = 0
next_obs = torch.Tensor(envs.reset()[0]).to(device)
next_done = torch.zeros(args.num_envs).to(device)

print("å¼€å§‹è®­ç»ƒ...")

# ä¸»è®­ç»ƒå¾ªç¯
for update in range(1, num_updates + 1):
    # å­¦ä¹ ç‡é€€ç«
    if args.anneal_lr:
        frac = 1.0 - (update - 1.0) / num_updates
        lrnow = frac * args.learning_rate
        optimizer.param_groups[0]["lr"] = lrnow

    update_episodes = 0
    update_rewards = []

    # é‡‡æ ·é˜¶æ®µ
    for step in range(args.num_steps):
        global_step += args.num_envs
        obs[step] = next_obs
        dones[step] = next_done

        # è·å–åŠ¨ä½œå’Œä»·å€¼
        with torch.no_grad():
            action, logprob, _, value = agent.get_action_and_value(next_obs)
            values[step] = value.flatten()

        actions[step] = action
        logprobs[step] = logprob

        # ç¯å¢ƒæ­¥è¿›
        next_obs, reward, done, truncated, info = envs.step(action.cpu().numpy())
        done = np.logical_or(done, truncated)

        rewards[step] = torch.tensor(reward).to(device).view(-1)
        next_obs = torch.Tensor(next_obs).to(device)
        next_done = torch.Tensor(done).to(device)

        # æ›´æ–°å›åˆç»Ÿè®¡
        for i in range(args.num_envs):
            episode_tracker.current_returns[i] += reward[i]
            episode_tracker.current_lengths[i] += 1

            if done[i]:
                ep_reward = episode_tracker.current_returns[i]
                ep_length = episode_tracker.current_lengths[i]

                episode_tracker.episode_rewards.append(ep_reward)
                episode_tracker.episode_lengths.append(ep_length)
                update_episodes += 1
                update_rewards.append(ep_reward)

                # TensorBoardè®°å½•
                writer.add_scalar("charts/episodic_return", ep_reward, global_step)
                writer.add_scalar("charts/episodic_length", ep_length, global_step)

                # é‡ç½®è®¡æ•°å™¨
                episode_tracker.current_returns[i] = 0
                episode_tracker.current_lengths[i] = 0

                if update_episodes <= 5:  # åªæ‰“å°å‰å‡ ä¸ªå›åˆé¿å…è¾“å‡ºè¿‡å¤š
                    print(f"  å›åˆå®Œæˆ - ç¯å¢ƒ{i}: å¥–åŠ±={ep_reward:.1f}, é•¿åº¦={ep_length}")

    # è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    with torch.no_grad():
        next_value = agent.get_value(next_obs).reshape(1, -1)
        advantages, returns = compute_advantages(
            rewards, values, dones, next_value, args.gamma, args.gae_lambda
        )

    # å‡†å¤‡è®­ç»ƒæ•°æ®
    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
    b_logprobs = logprobs.reshape(-1)
    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
    b_advantages = advantages.reshape(-1)
    b_returns = returns.reshape(-1)
    b_values = values.reshape(-1)

    # ç­–ç•¥æ›´æ–°
    b_inds = np.arange(args.batch_size)
    clipfracs = []

    for epoch in range(args.update_epochs):
        np.random.shuffle(b_inds)
        for start in range(0, args.batch_size, args.minibatch_size):
            end = start + args.minibatch_size
            mb_inds = b_inds[start:end]

            _, newlogprob, entropy, newvalue = agent.get_action_and_value(
                b_obs[mb_inds], b_actions.long()[mb_inds]
            )
            logratio = newlogprob - b_logprobs[mb_inds]
            ratio = logratio.exp()

            with torch.no_grad():
                old_approx_kl = (-logratio).mean()
                approx_kl = ((ratio - 1) - logratio).mean()
                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]

            mb_advantages = b_advantages[mb_inds]
            if args.norm_adv:
                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)

            # PPOç­–ç•¥æŸå¤±
            pg_loss1 = -mb_advantages * ratio
            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
            pg_loss = torch.max(pg_loss1, pg_loss2).mean()

            # å€¼å‡½æ•°æŸå¤±
            newvalue = newvalue.view(-1)
            if args.clip_vloss:
                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
                v_clipped = b_values[mb_inds] + torch.clamp(
                    newvalue - b_values[mb_inds], -args.clip_coef, args.clip_coef
                )
                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
                v_loss = 0.5 * v_loss_max.mean()
            else:
                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()

            entropy_loss = entropy.mean()
            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
            optimizer.step()

        if args.target_kl is not None and approx_kl > args.target_kl:
            break

    # è®°å½•è®­ç»ƒæŒ‡æ ‡
    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
    var_y = np.var(y_true)
    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y

    writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
    writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
    writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
    writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
    writer.add_scalar("losses/explained_variance", explained_var, global_step)

    # æ‰“å°è¿›åº¦
    if update_episodes > 0:
        avg_update_reward = np.mean(update_rewards)
        print(f"æ›´æ–° {update}: å®Œæˆ {update_episodes} ä¸ªå›åˆ, å¹³å‡å¥–åŠ±: {avg_update_reward:.2f}")

    if update % 10 == 0:
        total_episodes = len(episode_tracker.episode_rewards)
        if total_episodes > 0:
            recent_rewards = episode_tracker.episode_rewards[-50:]
            avg_reward = np.mean(recent_rewards)
            max_reward = max(episode_tracker.episode_rewards)

            print(f"\n=== æ›´æ–° {update}/{num_updates} ===")
            print(f"æ€»å›åˆæ•°: {total_episodes}")
            print(f"æœ€è¿‘50å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}")
            print(f"å†å²æœ€é«˜å¥–åŠ±: {max_reward:.2f}")
            print(f"ç­–ç•¥æŸå¤±: {pg_loss.item():.4f}")
            print(f"å€¼å‡½æ•°æŸå¤±: {v_loss.item():.4f}")

# å…³é—­ç¯å¢ƒå’Œè®°å½•å™¨
envs.close()
writer.close()

print(f"\nè®­ç»ƒå®Œæˆï¼")
print(f"æ€»å›åˆæ•°: {len(episode_tracker.episode_rewards)}")

# å¯è§†åŒ–ç»“æœ
if episode_tracker.episode_rewards:
    plt.figure(figsize=(15, 5))

    # å›åˆå¥–åŠ±
    plt.subplot(1, 3, 1)
    plt.plot(episode_tracker.episode_rewards, alpha=0.6, linewidth=1)
    if len(episode_tracker.episode_rewards) > 20:
        window = min(20, len(episode_tracker.episode_rewards) // 5)
        moving_avg = np.convolve(episode_tracker.episode_rewards, np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(episode_tracker.episode_rewards)), moving_avg, 'r-', linewidth=2)
    plt.title(f'å›åˆå¥–åŠ±è¶‹åŠ¿ (å…±{len(episode_tracker.episode_rewards)}å›åˆ)')
    plt.xlabel('å›åˆåºå·')
    plt.ylabel('å›åˆå¥–åŠ±')
    plt.grid(True, alpha=0.3)

    # å›åˆé•¿åº¦
    plt.subplot(1, 3, 2)
    plt.plot(episode_tracker.episode_lengths, alpha=0.6, linewidth=1)
    if len(episode_tracker.episode_lengths) > 20:
        window = min(20, len(episode_tracker.episode_lengths) // 5)
        moving_avg = np.convolve(episode_tracker.episode_lengths, np.ones(window) / window, mode='valid')
        plt.plot(range(window - 1, len(episode_tracker.episode_lengths)), moving_avg, 'r-', linewidth=2)
    plt.title('å›åˆé•¿åº¦è¶‹åŠ¿')
    plt.xlabel('å›åˆåºå·')
    plt.ylabel('å›åˆé•¿åº¦(æ­¥æ•°)')
    plt.grid(True, alpha=0.3)

    # å¥–åŠ±åˆ†å¸ƒ
    plt.subplot(1, 3, 3)
    bins = min(30, len(episode_tracker.episode_rewards) // 3)
    plt.hist(episode_tracker.episode_rewards, bins=bins, alpha=0.7, edgecolor='black')
    plt.title('å¥–åŠ±åˆ†å¸ƒ')
    plt.xlabel('å¥–åŠ±å€¼')
    plt.ylabel('é¢‘æ¬¡')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # ç»Ÿè®¡æŠ¥å‘Š
    rewards = episode_tracker.episode_rewards
    print(f"\n=== è®­ç»ƒç»Ÿè®¡æŠ¥å‘Š ===")
    print(f"æ€»å›åˆæ•°: {len(rewards)}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
    print(f"æ ‡å‡†å·®: {np.std(rewards):.2f}")
    print(f"æœ€é«˜å¥–åŠ±: {max(rewards):.2f}")
    print(f"æœ€ä½å¥–åŠ±: {min(rewards):.2f}")

    # æ€§èƒ½åˆ†æ
    last_50 = rewards[-50:] if len(rewards) >= 50 else rewards
    final_performance = np.mean(last_50)
    print(f"æœ€å{len(last_50)}å›åˆå¹³å‡: {final_performance:.2f}")

    if final_performance > 400:
        print("ğŸ‰ è®­ç»ƒè¡¨ç°ä¼˜ç§€ï¼æ™ºèƒ½ä½“å·²ç»æŒæ¡å¹³è¡¡æ†ä»»åŠ¡")
    elif final_performance > 200:
        print("ğŸ‘ è®­ç»ƒè¡¨ç°è‰¯å¥½ï¼Œè¿˜æœ‰æ”¹è¿›ç©ºé—´")
    elif final_performance > 100:
        print("ğŸ“ˆ è®­ç»ƒæœ‰è¿›å±•ï¼Œéœ€è¦ç»§ç»­ä¼˜åŒ–")
    else:
        print("ğŸ“š éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ—¶é—´")
else:
    print("âŒ æ²¡æœ‰æ”¶é›†åˆ°å›åˆæ•°æ®")


# æµ‹è¯•æ™ºèƒ½ä½“æ€§èƒ½
def evaluate_agent(agent, env_id, num_episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    env = gym.make(env_id)
    test_rewards = []
    test_lengths = []

    print(f"\nå¼€å§‹è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ ({num_episodes}å›åˆ)...")

    for episode in range(num_episodes):
        obs, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False

        while not done and steps < 1000:  # æœ€å¤§æ­¥æ•°é™åˆ¶
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)
            with torch.no_grad():
                action, _, _, _ = agent.get_action_and_value(obs_tensor)

            obs, reward, done, truncated, _ = env.step(action.cpu().item())
            total_reward += reward
            steps += 1
            done = done or truncated

        test_rewards.append(total_reward)
        test_lengths.append(steps)
        print(f"æµ‹è¯•å›åˆ {episode + 1:2d}: å¥–åŠ±={total_reward:6.1f}, æ­¥æ•°={steps:3d}")

    env.close()

    avg_reward = np.mean(test_rewards)
    avg_length = np.mean(test_lengths)

    print(f"\n=== æµ‹è¯•ç»“æœ ===")
    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
    print(f"å¹³å‡é•¿åº¦: {avg_length:.2f}")
    print(f"æˆåŠŸç‡: {sum(1 for r in test_rewards if r >= 400) / len(test_rewards) * 100:.1f}%")

    if avg_reward >= 450:
        print("ğŸ† æµ‹è¯•è¡¨ç°å“è¶Šï¼")
    elif avg_reward >= 350:
        print("ğŸ¯ æµ‹è¯•è¡¨ç°ä¼˜ç§€ï¼")
    elif avg_reward >= 200:
        print("ğŸ‘ æµ‹è¯•è¡¨ç°è‰¯å¥½ï¼")
    else:
        print("ğŸ“ˆ è¿˜æœ‰æå‡ç©ºé—´")

    return test_rewards, test_lengths


# è¿è¡Œæœ€ç»ˆæµ‹è¯•
test_rewards, test_lengths = evaluate_agent(agent, args.env_id, num_episodes=20)

print("\nè®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")