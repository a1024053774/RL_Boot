{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown-cartpole",
   "metadata": {},
   "source": [
    "# 使用DQN解决平衡杆问题 (CartPole)\n",
    "\n",
    "在这个Notebook中，我们将实现一个深度Q网络（DQN）来学习如何玩`CartPole-v1`游戏。目标是训练一个智能体，使其能够通过向左或向右移动小车来尽可能长时间地保持杆的平衡。\n",
    "\n",
    "**核心概念:**\n",
    "1.  **Q网络 (Q-Network)**: 我们将使用一个简单的全连接神经网络来近似Q函数。输入是环境的状态（4个值），输出是每个可能动作（向左或向右）的Q值。\n",
    "2.  **经验回放 (Experience Replay)**: 创建一个缓存区来存储过去的`(状态, 动作, 奖励, 下一状态, 是否结束)`五元组。训练时，从缓存区中随机采样一个小批量数据，这可以打破数据之间的相关性，使训练更稳定。\n",
    "3.  **目标网络 (Target Network)**: 使用一个独立的、更新较慢的目标网络来计算TD目标值，这有助于缓解训练过程中的振荡问题，提高稳定性。\n",
    "4.  **ε-greedy策略**: 在探索（随机选择动作）和利用（选择Q值最高的动作）之间取得平衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-markdown-cartpole",
   "metadata": {},
   "source": [
    "## 1. 环境设置\n",
    "\n",
    "首先，我们需要安装必要的库。`gymnasium`是环境库，`torch`是深度学习框架，`tensorboard`用于可视化。"
   ]
  },
  {
   "cell_type": "code",
   "id": "pip-install-cell-cartpole",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T13:12:41.329788Z",
     "start_time": "2025-07-28T13:12:38.145670Z"
    }
   },
   "source": [
    "!pip install gymnasium torch tensorboard --user"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: torch in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: tensorboard in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from tensorboard) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (1.67.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (3.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (4.25.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (75.8.2)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\developmentsoftware\\anaconda\\envs\\py311\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\10240\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "tensorboard-markdown-cartpole",
   "metadata": {},
   "source": [
    "## 2. 启动 TensorBoard (可选)\n",
    "\n",
    "为了可视化训练过程中的奖励变化和损失函数，您可以在终端中运行以下命令来启动TensorBoard。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard-launch-cell-cartpole",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 请在您的终端(Terminal/CMD)中运行此命令\n",
    "# tensorboard --logdir runs_cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "code-intro-markdown-cartpole",
   "metadata": {},
   "source": [
    "## 3. DQN 代码实现\n",
    "\n",
    "以下是DQN算法的完整Python代码。它被封装在一个`DQN`类中以便于理解和使用。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T13:26:37.529283Z",
     "start_time": "2025-07-28T13:14:53.596368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 设置环境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 设置设备 (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 定义经验回放的 Transition 结构\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "# Transition 用于存储每个经验的状态、动作、下一状态和奖励\n",
    "# Transition(state, action, next_state, reward) 可以用来存储一个完整的经验元组\n",
    "\n",
    "\n",
    "# 经验回放缓存区\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"保存一个 transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"从缓存区中随机采样一个批次\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# 定义Q网络 (一个简单的全连接网络)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "# --- 超参数 --- #\n",
    "BATCH_SIZE = 128         # 每次训练的样本数量\n",
    "GAMMA = 0.99             # 奖励折扣因子\n",
    "EPS_START = 0.9          # epsilon 的初始值\n",
    "EPS_END = 0.05           # epsilon 的最终值\n",
    "EPS_DECAY = 1000         # epsilon 的衰减速率\n",
    "TAU = 0.005              # 目标网络软更新的系数\n",
    "LR = 1e-4                # 学习率\n",
    "MEMORY_CAPACITY = 10000  # 经验回放缓存区大小\n",
    "NUM_EPISODES = 600       # 总共训练的回合数\n",
    "\n",
    "# 获取状态和动作空间的维度\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# 初始化网络、优化器和经验回放\n",
    "policy_net = QNetwork(n_observations, n_actions).to(device)\n",
    "target_net = QNetwork(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() # 目标网络不进行训练\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "writer = SummaryWriter(f\"runs_cartpole/dqn_{int(time.time())}\")\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"根据 epsilon-greedy 策略选择动作\"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    # 计算当前的 epsilon 值\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        # 利用: 选择Q值最高的动作\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # 探索: 随机选择一个动作\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"执行一步优化\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 创建非最终状态的掩码和对应的 next_states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # 计算当前状态的Q值: Q(s_t, a)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 计算下一状态的期望Q值: V(s_{t+1})\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    \n",
    "    # 计算期望的Q值 (TD Target)\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # 计算损失 (Huber Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    writer.add_scalar('Loss', loss.item(), steps_done)\n",
    "\n",
    "    # 优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "# --- 训练循环 --- #\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # 存入经验回放\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # 执行一步优化\n",
    "        optimize_model()\n",
    "\n",
    "        # 软更新目标网络权重\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            writer.add_scalar('Reward per Episode', total_reward, i_episode)\n",
    "            if (i_episode + 1) % 50 == 0:\n",
    "                print(f'Episode {i_episode+1}/{NUM_EPISODES}, Total Reward: {total_reward}')\n",
    "            break\n",
    "\n",
    "print('训练完成')\n",
    "env.close()\n",
    "writer.close()"
   ],
   "id": "3b75035f285fffc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode 50/600, Total Reward: 9.0\n",
      "Episode 100/600, Total Reward: 12.0\n",
      "Episode 150/600, Total Reward: 12.0\n",
      "Episode 200/600, Total Reward: 62.0\n",
      "Episode 250/600, Total Reward: 71.0\n",
      "Episode 300/600, Total Reward: 115.0\n",
      "Episode 350/600, Total Reward: 140.0\n",
      "Episode 400/600, Total Reward: 210.0\n",
      "Episode 450/600, Total Reward: 66.0\n",
      "Episode 500/600, Total Reward: 500.0\n",
      "Episode 550/600, Total Reward: 500.0\n",
      "Episode 600/600, Total Reward: 500.0\n",
      "训练完成\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-markdown-cartpole",
   "metadata": {},
   "source": [
    "## 4. 结果与总结\n",
    "\n",
    "训练完成后，可以通过TensorBoard查看每个回合（Episode）的总奖励曲线。\n",
    "\n",
    "如果训练成功，奖励曲线随着训练的进行而稳步上升，最终收敛在一个较高的水平（对于CartPole-v1，最高奖励是500）。\n",
    "\n",
    "运行tensorboard结果\n",
    "\n",
    "tensorboard --logdir runs_cartpole"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
